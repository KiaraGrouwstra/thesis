@online{software20,
  author = {Karpathy, Andrej},
  title = {Software 2.0},
  year = {2017},
  month = {Nov},
  type = {Blog},
  url = {https://medium.com/@karpathy/software-2-0-a64152b37c35},
  urldate = {2019-12-14}
}

@article{church1957applications,
  title={Applications of recursive arithmetic to the problem of circuit synthesis},
  author={Church, Alonzo},
  journal={Institute for Symbolic Logic, Cornell University},
  year={1957}
}

@article{gulwani2017program,
  title={Program synthesis},
  author={Gulwani, Sumit and
          Polozov, Oleksandr and
          Singh, Rishabh and
          others},
  journal={Foundations and Trends in Programming Languages},
  volume={4},
  number={1-2},
  pages={1--119},
  year={2017},
  doi={10.1561/2500000010},
  url={https://www.nowpublishers.com/article/Details/PGL-010}
}
% approaches: deductive (theorem provers, full specification) vs. grammar-based e.g. based on examples
% challenges: program space (full enumeration won't terminate), user intent (full specification unrealistic)
% dimensions: kind of constraints (user intent), search space of programs, search technique
% - user intent: logical specification (tricky to write), examples (potentially by property or iteratively), traces (step-by-step behavior on given input, e.g. intermediate states UI), natural language, partial programs, related programs (optimization, deobfuscation, inverses)
% - search space: expressiveness vs efficiency, imperative/functional, DSLs, control structure (e.g. partial program with holes)
% - search technique:
%   - enumerative search: enumerate/order/check, unscalable, breadth-first, bottom-up i.e. small first
%   - deduction: divide-and-conquer top-down search, depth-first, recursively reduce to sub-problems, propagating constraints
%   - constraint solving:
%     - constraint generation:
%       - invariant-based: full proof
%       - input-based: proof it works for certain inputs, typically paired with counter-example guided inductive synthesis (CEGIS) strategy
%       - path-based: in-between, works for inputs following some paths
%     - constraint resolution: SAT/SMT
%   - statistical: ML of probabilistic grammars, genetic programming, MCMC sampling, probabilistic inference
%   - combination: augment enumerative search or deduction by providing choices' likelihood
% general principles:
% - Second-Order Problem Reduction: reduce to first-order search problem using templates so we can use SMT/SAT solvers. sketches have holes fixed to e.g. integer, templates have holes for arbitrary expressions.
% - Oracle-Guided Synthesis: picking templates non-trivial, instead verify validity of candidate solutions.
%   - counterexample-guided inductive synthesis (CEGIS): either the candidate turns out valid, or we find a counter-example.
%   - oracle-guided inductive synthesis (OGIS) first uses simplified spec.
%   - Distinguishing Inputs: use loopless programs, check program distinguishability on concrete inputs.
%   - End-user confidence: e.g. find distinguishing input, ask user which program handles it correctly (if any).
%   - Syntactic Bias: restrict programs
%     - Sketching: int holes
%     - Syntax-Guided Synthesis (SyGuS): logical spec + syntactic template to constrain space. find candidates in grammar G satisfying spec φ in theory (language) T. competition SyGuS-Comp.
%     - DSL Design: factors:
%       - Balanced Expressivity
%       - Choice of Operators
%       - Naturalness
%       - Efficiency
%     - Optimization:
%       - Metrics: Program speed, Robustness, Naturalness/legibility
%       - Ranking:
%         - MCMC (smart hill climber)
%         - version space algebra (VSA)s: rankable repesentation of program space
%         - ml
%         - metasketches: ordered family of partial programs w/ cost function + gradient
% - Enumerative Search
%   - Enumerative Search
%     - Top-down Tree Search: like what I do for program generation
%     - Bottom-up Tree Search: dedupe bottom-up to prune search space (e.g. x+y vs y+x)
%   - Bidirectional Enumerative Search: forward search from inputs + backward from outputs.
%   - Offline Exhaustive Enumeration and Composition: hash programs by i/o.
% - Constraint Solving: encoding spec + syntactic restrictions in one formula.
%   - Component-based synthesis: use each component once.
%     - End-to-end SMT Encoding: ensure program:
%       - well-formed: syntax, deduped, acyclical
%       - component usage: respect types?
%       - specification: i/o correct
%     - Sketch generation and completion: less restrictive
%   - Solver-Aided Programming: extend language with SAT/SMT-powered constructs
%   - Inductive Logic Programming: synthesizing first-order rules/relations, not functional expressions.
% - Stochastic Search: learn a distribution over the space of programs in the hypothesis space conditioned on the specification, then sample
%   - Metropolis-Hastings Algorithm for Sampling Expressions: probability based on meeting specification
%   - Genetic Programming: natural selection/variation
%   - Machine Learning: learn a probability distribution over the space of programs conditioned by the specification
%   - Neural Program Synthesis: induction vs synthesis (e.g. NSPS)
% - Programming by Example (PBE)
%   - Problem Definition: use inputs + output examples/constraints. characterized by ease of use and ambiguity. find just the few good programs that match intent and generalize.
%   - Version Space Algebra: constraining DSL by picking only a few examples for each grammar production.
%   - Deduction-Based Techniques: push examples thru grammar top-down. aka divide-and-conquer search, top-down search, and backpropagation-based search. done by (a) inverse operator semantics, or (b) type-theoretic.
%     - Inverse Semantics: mitigating non-trivial inverse:
%       - witness function: pick only likely (reverse) inputs
%       - constraints: property of output
%       - Case split: Per-parameter decomposition of inverse semantics.
%     - Type-Based Perspective: propagate type refinements.
%   - Ambiguity Resolution: getting user intent harder with few samples in a rich DSL.
%     - Ranking: learning-to-rank any correct program on top.
%     - Active Learning: ask feedback by distinguishing inputs.
% - Future Work: Debuggability, Multi-modal input, Adaptivity, Statistical techniques, Scaling, Knowledge transfer, Industrialization
% [79] SmartSynth: synthesizing smartphone automation scripts from natural language

@article{nps,
  author    = {Neel Kant},
  title     = {Recent Advances in Neural Program Synthesis},
  journal   = {CoRR},
  volume    = {abs/1802.02353},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.02353},
  archivePrefix = {arXiv},
  eprint    = {1802.02353}
}
% Though there are still some firm purists in both, the symbolic and connectionist AI camps, the consensus is that neither approach is likely to solve program synthesis in isolation. What is important is that the two paradigms have \textbf{complementary} strengths and weaknesses in their abilities to \emph{represent} and \emph{learn} knowledge. Thus, it is natural to believe that a \textbf{hybrid system} is the most promising path forward.

% 5.2 Future Research Recommendations
% 6. Building more complete solutions will no doubt require greater \textbf{automation in learning}. Current methods rely heavily on hand-crafted curricula to incrementally challenge a model and improve generalization.

% My main take-away from this paper now is that most other neural techniques, on the basis of their sequence-to-sequence approach, lack an inherent incremental state and as such have to do a bunch of learning tasks just to get a sense of how to construct and update a (memorized) state so as to ultimately obtain a correct program.
% The way I see it NSPS just completely side-steps all these extra learning tasks, significantly simplifying the learning problem.

% aka Neural FlashFill
@article{nsps,
  author    = {Emilio Parisotto and
               Abdel{-}rahman Mohamed and
               Rishabh Singh and
               Lihong Li and
               Dengyong Zhou and
               Pushmeet Kohli},
  title     = {Neuro-Symbolic Program Synthesis},
  journal   = {CoRR},
  volume    = {abs/1611.01855},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01855},
  archivePrefix = {arXiv},
  eprint    = {1611.01855}
}
% no code; (1) cross correlation I/O network embeds in/out examples, (2) Recursive-Reverse-Recursive Neural Network (R3NN) incrementally synthesizes AST. R3NN has node representations, then calculates representations bottom-up then top-down to get representations for the next hole to fill. tests on flashfill. ops Concat/ConstStr/SubStr/Position/Direction/Regex.

@article{alphanpi,
  author    = {Thomas Pierrot and
               Guillaume Ligner and
               Scott E. Reed and
               Olivier Sigaud and
               Nicolas Perrin and
               Alexandre Laterre and
               David Kas and
               Karim Beguir and
               Nando de Freitas},
  title     = {Learning Compositional Neural Programs with Recursive Tree Search and Planning},
  journal   = {CoRR},
  volume    = {abs/1905.12941},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.12941},
  archivePrefix = {arXiv},
  eprint    = {1905.12941}
}
% [blog](https://www.instadeep.com/research-article/towards-compositionality-in-deep-reinforcement-learning/)
% Neural Programmer-Interpreter (NPI) + AlphaZero = AlphaNPI. tests on sorting/hanoi tasks. ops as NPI.

% NMT
@inproceedings{kalchbrenner2013recurrent,
  title={Recurrent continuous translation models},
  author={Kalchbrenner, Nal and
          Blunsom, Phil},
  editor={},
  booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages={1700--1709},
  url={https://www.aclweb.org/anthology/D13-1176.pdf},
  year={2013}
}

@article{npi,
  title={Neural programmer-interpreters},
  author={Reed, Scott and
          De Freitas, Nando},
  url={https://arxiv.org/abs/1511.06279},
  journal={arXiv preprint arXiv:1511.06279},
  year={2015}
}
% DeepMind, seq2seq. tests on add/sort / canonicalizing 3d models. ops reset/bubble/rshift/lshift/compswap/ptr1l/ptr1r/ptr2l/ptr2r/swap/stop.

% holes
@inproceedings{hashimoto1997typed,
  title={A typed context calculus},
  author={Hashimoto, Masatomo and
          Ohori, Atsushi},
  editor={},
  booktitle={Theoretical Computer Science},
  year={1997},
  url={https://www.sciencedirect.com/science/article/pii/S0304397500001742},
  doi={10.1016/S0304-3975(00)00174-2},
  organization={Citeseer}
}

@inproceedings{deepproblog,
  title={Deepproblog: Neural probabilistic logic programming},
  author={Manhaeve, Robin and
          Dumancic, Sebastijan and
          Kimmig, Angelika and
          Demeester, Thomas and
          De Raedt, Luc},
  editor={},
  booktitle={Advances in Neural Information Processing Systems},
  url={https://arxiv.org/abs/1805.10872},
  pages={3749--3759},
  year={2018}
}
% differentiable [ProbLog](https://dtai.cs.kuleuven.be/problog/).
% Emile: less relevant cuz untyped? tests on add/sort/algebra on pics/list/text. induction, no ops.

@inproceedings{problog,
  title={ProbLog: A Probabilistic Prolog and Its Application in Link Discovery},
  author={De Raedt, Luc and
          Kimmig, Angelika and
          Toivonen, Hannu},
  editor={},
  url={https://dtai.cs.kuleuven.be/problog/},
  booktitle={IJCAI},
  volume={7},
  pages={2462--2467},
  year={2007},
  organization={Hyderabad}
}

@article{dilp,
  title={Learning explanatory rules from noisy data},
  author={Evans, Richard and
          Grefenstette, Edward},
  url={https://arxiv.org/abs/1711.04574},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={1--64},
  year={2018}
}
% DeepMind, [code](https://github.com/ai-systems/DILP-Core)
% Differentiable Inductive Logic Programming (∂ILP) = ILP (data efficient) + NNs' ability to handle ambiguity. trained on various symbolic tasks. uses given template so just handles induction. check if this can do types?
% see pic on p.31. tests on [20 symbolic tasks](https://arxiv.org/pdf/1711.04574.pdf#page=27) (see appendix G: arithmetic, lists, family tree, graphs). task-dependent ops. not sure I can add value here if the template is set.

% #neural-guided-deductive-search
@article{deepcoder,
  title={Deepcoder: Learning to write programs},
  author={Balog, Matej and
          Gaunt, Alexander L and
          Brockschmidt, Marc and
          Nowozin, Sebastian and
          Tarlow, Daniel},
  url={https://www.microsoft.com/en-us/research/publication/deepcoder-learning-write-programs/},
  journal={arXiv preprint arXiv:1611.01989},
  year={2016}
}
% (repos [1](https://github.com/HiroakiMikami/deep-coder), [2](https://github.com/dkamm/deepcoder), [3](https://github.com/amitz25/PCCoder))
% train a neural network to predict properties of the program that generated the outputs from the inputs to augment search techniques, e.g. enumerative search, SMT-based solver, depth-first search (DFS), 'sort and add' enumeration, sketch, λ^2. tested on functional programs. does not use a node tree though, so not amenable to my type-based approach.

% #neural-guided-deductive-search
@article{kalyan2018neural,
  title={Neural-guided deductive search for real-time program synthesis from examples},
  author={Kalyan, Ashwin and
          Mohta, Abhishek and
          Polozov, Oleksandr and
          Batra, Dhruv and
          Jain, Prateek and
          Gulwani, Sumit},
  url={https://www.microsoft.com/en-us/research/publication/neural-guided-deductive-search-real-time-program-synthesis-examples/},
  journal={arXiv preprint arXiv:1804.01186},
  year={2018}
}
% https://www.microsoft.com/en-us/research/blog/neural-guided-deductive-search-best-worlds-approach-program-synthesis/
% - combines symbolic logic + statistical models by 'branch & bound' from combinatorial optimization.
% - deductive search: decompose synthesis into sub-problems, Markovian so can do supervised learning.
% - NGDS: find which sub-problems are useful, predict program generalization by LSTM scoring DSL production rules on i/o.
% - evaluated on string transformation (PROSE) against RobustFill / DeepCoder.
% - deductive search seems kinda like pruning by type reasoning, though here it's learned. I guess the stronger the types tho, the less needs learning.
% - train separate models for different DSL levels, symbols, or even productions.
% - I *think* this seems iterative top-down, so seems applicable for me, tho no code.
% - this made me realize you may want to factor in existing operations on i/o to simplify the problem for the given sub-node.
% - sub-problems mentioned sounds relevant, and their previous paper FlashMeta (which this builds on) is mentioned in a [repo](https://github.com/reudismam/Refazer/blob/58f68f2c3f93e148a50f2eb09879a7bbe7fd6e3a/ProgramSynthesis/ProseFunctions/RelativePositioning/RightWitnessFunction.cs), though I'm not sure where this is imported from.
% - the used framework PROSE [offers holes](https://microsoft.github.io/prose/documentation/prose/usage/), implying they support the top-down incremental approach I'm hoping these papers are using.
% - from my understanding, from this paper (and the [FlashMeta paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/oopsla15-pbe.pdf)), my impression is that the approach used is more similar to bottom-up (though not narrowing down on one option at any level), prioritizing productions (then higher-level constructs) to direct a search. whereas I like the compositional nature of the bottom-up approach, for type purposes I appreciate that the top-down approach (as used by the NSPS paper) is type-bound by the desired input/output types.
% - PROSE's witness functions for constraint propagation look super interesting though! I kinda want this both for types and for input/output examples...

@inproceedings{flashmeta,
  title={FlashMeta: a framework for inductive program synthesis},
  author={Polozov, Oleksandr and
          Gulwani, Sumit},
  editor={},
  url={https://dl.acm.org/citation.cfm?id=2814310},
  booktitle={ACM SIGPLAN Notices},
  volume={50},
  number={10},
  pages={107--126},
  year={2015},
  doi={10.1145/2814270.2814310},
  organization={ACM}
}

@article{rnngrammars,
  title={Recurrent neural network grammars},
  author={Dyer, Chris and
          Kuncoro, Adhiguna and
          Ballesteros, Miguel and
          Smith, Noah A},
  url={https://arxiv.org/abs/1602.07776},
  journal={arXiv preprint arXiv:1602.07776},
  year={2016}
}
% probabilistic model of phrase-structure trees that can be trained generatively and used as a language model or a parser. seems aimed at language models, i.e. mimicking existing usage, which for program synthesis might be secondary to finding programs properly mapping input to output.

@article{bunel2018leveraging,
  title={Leveraging grammar and reinforcement learning for neural program synthesis},
  author={Bunel, Rudy and
          Hausknecht, Matthew and
          Devlin, Jacob and
          Singh, Rishabh and
          Kohli, Pushmeet},
  url={https://arxiv.org/abs/1805.04276},
  journal={arXiv preprint arXiv:1805.04276},
  year={2018}
}
% supervised + RL to generate semantically correct programs. train to generate syntactically correct programs that fulfill the specification. only applies for sequence approach, which doesn't match the trees I want...

@inproceedings{typedmil,
  title={Typed meta-interpretive learning of logic programs},
  author={Morel, Rolf and
          Cropper, Andrew and
          Ong, C-H Luke},
  editor={},
  url={https://www.researchgate.net/profile/Andrew_Cropper/publication/331100541_Typed_meta-interpretive_learning_of_logic_programs/links/5c65c7bd299bf1d14cc75a39/Typed-meta-interpretive-learning-of-logic-programs.pdf},
  booktitle={European Conference on Logics in Artificial Intelligence},
  pages={198--213},
  year={2019},
  doi={10.1007/978-3-030-19570-0_13},
  organization={Springer}
}
% Prolog/Python [code](https://github.com/rolfmorel/jelia19-typedmil)
% typed MIL (~= inductive logic programming (ILP)) reduces space by a cubic factor. tests on [list manipulation](https://github.com/rolfmorel/jelia19-typedmil/tree/master/experiments/03-evaluation). [ops](https://github.com/rolfmorel/jelia19-typedmil/blob/master/experiments/03-evaluation/nestedincr/gen_test_prolog.py#L43-L67). types: list<T>, int, char.

@article{wang2018predicting,
  title={Predicting Haskell Type Signatures From Names},
  author={Wang, Bowen},
  journal = {Semantic Scholar},
  journaltitle = {Semantic Scholar},
  url={https://people.cs.uchicago.edu/~rchugh/static/theses/bowen-wang-thesis.pdf},
  year={2018}
}
% encode name/context, recursively select (non-)arrow

@inproceedings{lambda2,
  title={Synthesizing data structure transformations from input-output examples},
  author={Feser, John K and
          Chaudhuri, Swarat and
          Dillig, Isil},
  editor={},
  url={https://www.cs.utexas.edu/~isil/pldi15b.pdf},
  booktitle={ACM SIGPLAN Notices},
  volume={50},
  number={6},
  pages={229--239},
  year={2015},
  doi={10.1145/2813885.2737977},
  organization={ACM}
}
% 2015, 145 cites, [code](https://github.com/jfeser/L2), λ^2
% OCaml+Python PBE from 7 combinators by inductive generalisation + (limited) deduction + enumerative search. typed, incremental, holes. old functional synthesis paper that got good improvement from types. tests on 40 data structure manipulation tasks (lists, trees, nested). ops: map, maptree, filter, foldl, foldr, foldtree, recl. 41 ops (lists, trees, nested).

@article{myth,
  title={Type-and-example-directed program synthesis},
  author={Osera, Peter-Michael and
          Zdancewic, Steve},
  url={https://dl.acm.org/citation.cfm?id=2738007},
  journal={ACM SIGPLAN Notices},
  volume={50},
  number={6},
  pages={619--630},
  doi={10.1145/2813885.2738007},
  year={2015}
}
% 100 cites, [code](https://github.com/psosera/thesis/blob/master/code/LambdaTermsCounts.hs)
% Haskell/OCaml PBE by types + i/o. shows own synthesis/typecheck rules, library. tests on 40 (figure #9) + 5 (#10) functions ([details](https://github.com/psosera/thesis/blob/master/content/benchmark-suite.tex)). code repo unreleased.
% list/tree categories polymorphic, bool/nat aren't. eval during enum, standardize on beta-normal, typechecks partial functions, lists relevant math. top-down generates refinement trees by cycling through steps adding a type (type refinement) or potential values (guessing enumeratively) by examples. essentially constructs a simple idea that works for one example, check which other examples fail, and which parts of the program should be refined to fix those. pattern matches ADTs like lists (nil). enforces recursion. ppt claims ml-syn incomplete? lists several directions for future work. seems like a good paper, I should look for citing papers in case any has open-sourced a similar solution. should recheck if this does 'holes' though, as it seemed to generate complete programs?

@inproceedings{scout,
  title={Programming assistance for type-directed programming},
  author={Osera, Peter-Michael},
  editor={},
  url={https://www.cs.grinnell.edu/~osera/publications/osera-tyde-2016.pdf},
  booktitle={Proceedings of the 1st International Workshop on Type-Driven Development},
  pages={56--57},
  year={2016},
  doi={10.1145/2976022.2976027},
  organization={ACM}
}
% no code?, bridge auto-completion / program synthesis with interactive programming assistant for type-directed programming.

@inproceedings{synquid,
  title={Program synthesis from polymorphic refinement types},
  author={Polikarpova, Nadia and
          Kuraj, Ivan and
          Solar-Lezama, Armando},
  editor={},
  url={https://dl.acm.org/citation.cfm?id=2908093},
  booktitle={ACM SIGPLAN Notices},
  volume={51},
  number={6},
  pages={522--538},
  year={2016},
  doi={10.1145/2980983.2908093},
  organization={ACM}
}
% [code](https://bitbucket.org/nadiapolikarpova/synquid)
% code from refinement types. synthesize Haskell-like Synquid by SMT-solver from modular refinement type reconstruction to allow type-checking partial programs using liquid types.

@mastersthesis{tamandu,
  title={Top-Down Inductive Synthesis with Higher-Order Functions},
  author={Maximova, Alexandra},
  url={https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/155821/eth-49631-01.pdf},
  year={2016},
  school={ETH Z{\"u}rich}
}
% [code](https://github.com/shasfin/ml4fp2016)
% PBE in toy language from OCaml w/ large library (37 components), based on best-first enumeration combined with type-based pruning, evaluated on 23 functions (table 4.2).

@article{hornclauses,
  title={Functional Program Synthesis from Relational Specifications},
  author={Nakao, Shuu and
          Satake, Yuki and
          Amano, Hiroshi},
  url={http://jssst.or.jp/files/user/taikai/2017/PPL/ppl3-1.pdf},
  journal={Japan Computer Science Conference Journal},
  volume={34},
  pages={207--220},
  year={2017}
}
  title={関係的仕様からの関数型プログラム合成},
  author={中尾收 and
          佐竹佑規 and
          海野広志},
  journal={日本ソフトウェア科学会大会論文集},
% Haskell synthesis using Horn clauses.

@phdthesis{gissurarson2018suggesting,
  title={Suggesting Valid Hole Fits for Typed-Holes in Haskell},
  author={Gissurarson, Matth{\'\i}as P{\'a}ll},
  url={https://www.mpg.is/papers/gissurarson2018suggesting-msc.pdf},
  year={2018},
  school={Master’s thesis. Chalmers University of Technology, University of Gothenburg~…}
}
% [code](https://github.com/Tritlo/ExampleHolePlugin)
% integrates hole filler into GHC. mentions synthesizers insynth (JVM), [djinn](https://github.com/augustss/djinn/) (haskell, non-polymorphic). dependently typed languages: type inference undecidable. mentions hole-filling for agda and idris. great explanation of program synthesis on typed holes at 3.1.

@online{djinn,
  author = {Lennart Augustsson},
  title = {djinn: Generate Haskell code from a type},
  year = {2014},
  month = {Sep},
  url = {http://hackage.haskell.org/package/djinn},
  urldate = {2020-01-24}
}

@article{guospeeding,
  title={Speeding up Type-Driven Program Synthesis with Polymorphic Succinct Types},
  journaltitle = {ICFP},
  url={https://icfp18.sigplan.org/getImage/orig/icfp18src-zheng-guo.pdf},
  year={2018},
  author={Guo, Zheng}
}
% [code](https://github.com/aaronguo1996/Synquid)
% generalize succinct types for polymorphism. to find which types are used under polymorphism, abstract the types into coarse-grained succinct types, unordered param type sets describing function i/o.

@inproceedings{eguchi2018automated,
  title={Automated Synthesis of Functional Programs with Auxiliary Functions},
  author={Eguchi, Shingo and
          Kobayashi, Naoki and
          Tsukada, Takeshi},
  editor={},
  url={https://www-kb.is.s.u-tokyo.ac.jp/~koba/papers/aplas18-long.pdf},
  booktitle={Asian Symposium on Programming Languages and Systems},
  pages={223--241},
  year={2018},
  doi={10.1007/978-3-030-02768-1_13},
  organization={Springer}
}
% extension of Synquid to enable top-down iterative synthesis of programs with auxiliary functions using holes.

@inproceedings{scythe,
  title={Constraint-based type-directed program synthesis},
  author={Osera, Peter-Michael},
  editor={},
  url={https://arxiv.org/abs/1907.03105},
  booktitle={Proceedings of the 4th ACM SIGPLAN International Workshop on Type-Driven Development},
  pages={64--76},
  year={2019},
  organization={ACM}
}
% no code?, synthesis from type. “infer while enumerate” approach to polymorphic type instantiation. uses WIP Haskell synthesis tool Scythe. no task!

@inproceedings{temporalstreamlogic,
  title={Temporal stream logic: Synthesis beyond the bools},
  author={Finkbeiner, Bernd and
          Klein, Felix and
          Piskac, Ruzica and
          Santolucito, Mark},
  editor={},
  url={https://arxiv.org/abs/1712.00246},
  booktitle={International Conference on Computer Aided Verification},
  pages={609--629},
  year={2019},
  organization={Springer}
}
% CEGAR-based synthesis (from formal spec) separating control and data. benchmarks: synthesized music player Android app, controller for autonomous vehicle in the Open Race Car Simulator (TORCS).

@article{combopt,
  title={Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon},
  author={Bengio, Yoshua and
          Lodi, Andrea and
          Prouvost, Antoine},
  url={https://arxiv.org/abs/1811.06128},
  journal={arXiv preprint arXiv:1811.06128},
  year={2018}
}
% - is it useful to frame program synthesis as a [Combinatorial Optimization](https://paperswithcode.com/task/combinatorial-optimization) problem (not unlike the TSP)?
% - how do they differ? -- CO implies a loss/reward per configuration. for ML I should need a loss anyway tho, so this seems reasonable. the sub-field of linear programming requires linear relationships, which I doubt apply here.
% - what are the differences between their algorithms?

@inproceedings{houdini,
  title={Houdini: Lifelong learning as program synthesis},
  author={Valkov, Lazar and
          Chaudhari, Dipak and
          Srivastava, Akash and
          Sutton, Charles and
          Chaudhuri, Swarat},
  editor={},
  url={https://arxiv.org/abs/1804.00218},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8687--8698},
  year={2018}
}
% [code](https://github.com/capergroup/houdini)
% NNs as typed differentiable functional programs to compose a library of neural functions to mix perception and procedural reasoning. library: NN, compose, map, fold, conv, on list/graph/tensor of bool/real. synth: generate (heuristically-guided top-down iterative refinement) + learn, evaluated on lifelong learning. suggests incorporating search strategies from NAS. not actually doing program synthesis tho... tests on several lifelong learning tasks (mix perceptual/algorithmic knowledge): recognize_digit, classify_digit, is_toy, regress_speed, regress_mnist, count_digit, count_toy, sum_digits, shortest_path_street, shortest_path_mnist. ops: hole, map, fold, convolution, compose.

@article{idris,
  title={Idris, a general-purpose dependently typed programming language: Design and implementation},
  author={Brady, Edwin},
  url={https://eb.host.cs.st-andrews.ac.uk/drafts/impldtp.pdf},
  journal={Journal of functional programming},
  volume={23},
  number={5},
  pages={552--593},
  doi={10.1017/S095679681300018X},
  year={2013}
}
% [code](https://github.com/idris-lang/Idris-dev)
% covers language design including holes (section 4).

@book{brady2017type,
  title={Type-driven development with Idris},
  author={Brady, Edwin},
  year={2017},
  publisher={Manning Publications Company}
}

@article{forth,
  author    = {Sebastian Riedel and
                Matko Bosnjak and
                Tim Rockt{\"{a}}schel},
  title     = {Programming with a Differentiable Forth Interpreter},
  journal   = {CoRR},
  volume    = {abs/1605.06640},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.06640},
  archivePrefix = {arXiv},
  eprint    = {1605.06640}
}

% 78 cites
@article{terpret,
  title={Terpret: A probabilistic programming language for program induction},
  author={Gaunt, Alexander L and
          Brockschmidt, Marc and
          Singh, Rishabh and
          Kushman, Nate and
          Kohli, Pushmeet and
          Taylor, Jonathan and
          Tarlow, Daniel},
  journal={arXiv preprint arXiv:1608.04428},
  url={https://arxiv.org/abs/1608.04428},
  year={2016}
}

@inproceedings{rosette,
  title={Growing solver-aided languages with rosette},
  author={Torlak, Emina and Bodik, Rastislav},
  editor={},
  booktitle={Proceedings of the 2013 ACM international symposium on New ideas, new paradigms, and reflections on programming \& software},
  pages={135--152},
  year={2013},
  url={https://homes.cs.washington.edu/~emina/pubs/rosette.onward13.pdf},
  doi={10.1145/2509578.2509586},
  organization={ACM}
}

@article{lambdacalculus,
  title={A set of postulates for the foundation of logic},
  author={Church, Alonzo},
  journal={Annals of mathematics},
  pages={346--366},
  doi={10.2307/1968337},
  year={1932}
}

@article{currying,
  title={{\"U}ber die Bausteine der mathematischen Logik},
  author={Sch{\"o}nfinkel, Moses},
  journal={Mathematische annalen},
  volume={92},
  number={3},
  pages={305--316},
  year={1924}
}

@techreport{fortran95,
  type = {Standard},
  key = {ISO/IEC 1539-1:1997},
  url = {https://www.iso.org/standard/26933.html},
  author = {ISO},
  year = {1997},
  month = {Dec},
  title = {Information technology --- Programming languages --- Fortran - Part 1: Base language},
  address = {Geneva, CH},
  institution = {International Organization for Standardization}
}

@article{abstractsyntaxnetworks,
  author    = {Maxim Rabinovich and
                Mitchell Stern and
                Dan Klein},
  title     = {Abstract Syntax Networks for Code Generation and Semantic Parsing},
  journal   = {CoRR},
  volume    = {abs/1704.07535},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.07535},
  archivePrefix = {arXiv},
  eprint    = {1704.07535}
}
% uses LSTM to synthesize ASTs from natural language

@inproceedings{shen2019using,
  title={Using active learning to synthesize models of applications that access databases},
  author={Shen, Jiasi and Rinard, Martin C},
  editor={},
  booktitle={Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages={269--285},
  year={2019},
  doi={10.1145/3314221.3314591},
  organization={ACM}
}

@article{shi2019frangel,
  title={FrAngel: component-based synthesis with control structures},
  author={Shi, Kensen and Steinhardt, Jacob and Liang, Percy},
  journal={Proceedings of the ACM on Programming Languages},
  volume={3},
  number={POPL},
  pages={73},
  year={2019},
  doi={10.1145/3290386},
  url = {https://dl.acm.org/doi/abs/10.1145/3290386}
}

@online{architecture,
  title = {Program synthesis for declarative building design},
  author = {Andrew Zukoski and
            Drew Wolpert},
  year = {2017},
  month = {Sep},
  url = {https://youtu.be/yJW--wNMv1M},
  urldate = {2019-12-16}
}

@article{lenses,
  title={Synthesizing bijective lenses},
  author={Miltner, Anders and
          Fisher, Kathleen and
          Pierce, Benjamin C and
          Walker, David and
          Zdancewic, Steve},
  journal={Proceedings of the ACM on Programming Languages},
  volume={2},
  number={POPL},
  pages={1},
  doi={10.1145/3158089},
  url={https://dl.acm.org/doi/abs/10.1145/3158089},
  year={2017}
}

@article{neuralmachinetranslation,
  author    = {Taro Sekiyama and
                Akifumi Imanishi and
                Kohei Suenaga},
  title     = {Towards Proof Synthesis Guided by Neural Machine Translation for Intuitionistic Propositional Logic},
  journal   = {CoRR},
  volume    = {abs/1706.06462},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.06462},
  archivePrefix = {arXiv},
  eprint    = {1706.06462}
}

% the paper is type-based, not by itself using examples.
% source={https://github.com/hazelgrove/hazelnut-dynamics-agda},
@article{omar2019live,
  title={Live functional programming with typed holes},
  author={Omar, Cyrus and
          Voysey, Ian and
          Chugh, Ravi and
          Hammer, Matthew A},
  journal={Proceedings of the ACM on Programming Languages},
  volume={3},
  number={POPL},
  pages={14},
  year={2019},
  doi={10.1145/3290327},
  url={https://arxiv.org/abs/1805.00155}
}
% followup-source={https://github.com/hazelgrove/hazelnut-myth-agda},
% "the mechanization of our ongoing work on type+example synthesis with big-step hazelnut dynamics."
% the code looks as abstract as the paper -- I don't see anything about examples though, which are not mentioned in the paper either...

@inproceedings{odena2020learning,
  title={Learning to Represent Programs with Property Signatures},
  author={Augustus Odena and
          Charles Sutton},
  editor={},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://arxiv.org/abs/2002.09030}
}

@article{das5,
  title={A medium-scale distributed system for computer science research: Infrastructure for the long term},
  author={Bal, Henri and
          Epema, Dick and
          de Laat, Cees and
          van Nieuwpoort, Rob and
          Romein, John and
          Seinstra, Frank and
          Snoek, Cees and
          Wijshoff, Harry},
  journal={Computer},
  volume={49},
  number={5},
  pages={54--63},
  year={2016},
  url={https://www.cs.vu.nl/das/das2016.pdf}
}

@article{deville1994logic,
  title={Logic program synthesis},
  author={Deville, Yves and
          Lau, Kung-Kiu},
  journal={The Journal of Logic Programming},
  volume={19},
  pages={321--350},
  url={https://core.ac.uk/download/pdf/82617549.pdf},
  year={1994}
}
% There are three main approaches to logic program synthesis by formal methods:
% \item In the constructive approach, a conjecture based on the specification is constructively proved, and from this proof, the specified program is extracted. We call this approach constructive synthesis.
% \item A more direct approach is to deduce clauses for the specified program directly from the specification. We shall call this approach deductive synthesis. Another approach can induce a program from a partial specification of the program.
% \item The program is a generalization of the partial specification. We shall call this approach inductive synthesis.

% out of trace-/model-based I guess NPS is model-based while e.g. Osera's approach seemed more trace-based:
% ~\citep{deville1994logic}: Within the methods for inductive program synthesis, one can distinguish between the trace-based approach and the model-based approach.
% In the trace-based approach, example traces are first generated. A trace is a sequence of instructions executed by an unknown program on some given input data. Then the traces are generalized into a program. This program may be obtained by folding, matching, and generalizing the traces. Generalization is required since traces are related to some specific inputs; folding is required in order to form loops and recursion.
% In the model-based approach, synthesis aims at constructing a finite axiomatization of a model of the examples. It thus makes an intensional representation of a relation (i.e., a program) from the given (incomplete) extensional representation (i.e., the examples). The model-based approach to inductive synthesis of logic program is better known as Inductive Logic Programming (ILP).
% 6.2. Schema-Guided Program Construction: template-based
% basically every node-picking step I do is an instance of this.

@misc{bodik2013algorithmic,
  title={Algorithmic program synthesis: introduction},
  author={Bod{\'\i}k, Rastislav and
          Jobstmann, Barbara},
  year={2013},
  doi={10.1007/s10009-013-0287-9},
  url={https://link.springer.com/article/10.1007/s10009-013-0287-9}
}
% We divide the field into reactive synthesis, which is concerned with automata-theoretic techniques for controllers that handle an infinite stream of requests, and functional synthesis, which produces programs consuming finite input.
% axiomatic synthesizers (improve basic program incrementally by axioms, no partial programs) vs. syntactically derived synthesizers (gradually fill parameterizable template program using a grammar), origins in inductive inference of functions and formal languages from examples.
% Inductive programming Synthesis of programs from incomplete specifications presented as positive and negative examples has been developed for functional programs [92] and logic programs.
% generate-and-test
% mashup of web services: Planning and synthesis techniques have been successfully used to compose web services, e.g., in [10,37].
% RL by synthesis: The programming language aLisp (agent Lisp) by Andre et al. [4,130] was designed for expressing prior knowledge in hierarchical reinforcement learning of agent policies.
% Huynh et al. [82] developed an algorithm that synthesizes [a web scraping] extraction program from user demonstration.

% neural tree transducer (NTT): tree-to-tree learning by top-down depth-first context-sensitive tree decoder paired with recursive neural encoders.
@article{sedoc2018neural,
  title={Neural Tree Transducers for Tree to Tree Learning},
  author={Sedoc, Jo{\~a}o and
          Foster, Dean and
          Ungar, Lyle},
  url={https://openreview.net/forum?id=rJBwoM-Cb&noteId=rJBwoM-Cb},
  journaltitle = {Open Review},
  year={2018}
}

% #neural-guided-deductive-search
@article{camacho2019towards,
  title={Towards Neural-Guided Program Synthesis for Linear Temporal Logic Specifications},
  author={Camacho, Alberto and
          McIlraith, Sheila A},
  journal={arXiv preprint arXiv:1912.13430},
  url={https://arxiv.org/abs/1912.13430},
  year={2019}
}
% LTL by RL. synthesize a strategy that reacts to a potentially adversarial environment while ensuring that all executions satisfy a Linear Temporal Logic (LTL) specification.

@article{fan2019adaptive,
  title={Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation},
  author={Fan, Xinjie and
          Zhang, Yizhe and
          Wang, Zhendong and
          Zhou, Mingyuan},
  journal={arXiv preprint arXiv:1912.13151},
  url={https://arxiv.org/abs/1912.13151},
  year={2019}
}
% adapt to contextual generation of categorical sequences a policy gradient estimator

@article{yin2017syntactic,
  title={A syntactic neural model for general-purpose code generation},
  author={Yin, Pengcheng and
          Neubig, Graham},
  journal={arXiv preprint arXiv:1704.01696},
  url={https://arxiv.org/abs/1704.01696},
  year={2017}
}
% a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge for program synthesis from natural-language input

@article{xu2019neural,
  title={Neural Program Synthesis By Self-Learning},
  author={Xu, Yifan and
          Dai, Lu and
          Singh, Udaikaran and
          Zhang, Kening and
          Tu, Zhuowen},
  journal={arXiv preprint arXiv:1910.05865},
  url={https://arxiv.org/abs/1910.05865},
  year={2019}
}
% neural program synthesis algorithm learned via self-learning RL that explores the large code space efficiently.

@article{young2019learning,
  title={Learning neurosymbolic generative models via program synthesis},
  author={Young, Halley and
          Bastani, Osbert and
          Naik, Mayur},
  journal={arXiv preprint arXiv:1901.08565},
  url={https://arxiv.org/abs/1901.08565},
  year={2019}
}
% incorporate programs representing global structure into the generative model

@inproceedings{chen2019neural,
  title={Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension},
  author={Chen, Xinyun and
          Liang, Chen and
          Yu, Adams Wei and
          Zhou, Denny and
          Song, Dawn and
          Le, Quoc V},
  editor={},
  booktitle={International Conference on Learning Representations},
  url={https://openreview.net/forum?id=ryxjnREFwH&noteId=ryxjnREFwH},
  year={2019}
}
% Neural Symbolic Reader (NeRd): reader (e.g., BERT) to encode the passage/question + programmer (e.g., LSTM) to generate a program to produce the answer.

@article{chen2017towards,
  title={Towards synthesizing complex programs from input-output examples},
  author={Chen, Xinyun and
          Liu, Chang and
          Song, Dawn},
  journal={arXiv preprint arXiv:1706.01284},
  url={https://arxiv.org/abs/1706.01284},
  year={2017}
}
% new task: learn parser from input + parse trees. tackle by: (1) use non-differentiable machine to restrict search space; (2) recursion; (3) RL to operate the machine.

@inproceedings{shin2019program,
  title={Program synthesis and semantic parsing with learned code idioms},
  author={Shin, Eui Chul and
          Allamanis, Miltiadis and
          Brockschmidt, Marc and
          Polozov, Alex},
  editor={},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10824--10834},
  url={http://papers.nips.cc/paper/9265-program-synthesis-and-semantic-parsing-with-learned-code-idioms},
  year={2019}
}
% Patois: let a neural program synthesizer interleave high-/low-level reasoning by automatically mining common code idioms from a given corpus.

@online{polozov,
  author = {Polozov, Alex},
  title = {Program Synthesis in 2017-18},
  year = {2018},
  month = {Jul},
  url={https://alexpolozov.com/blog/program-synthesis-2018/},
  urldate = {2020-02-23}
}
% Program Synthesis in 2017-18
% Neural-guided search
% Sketch generation
% Graph neural networks
% Datasets
% Notable mentions: differentiable FlashFill, Conflict-Driven Learning, Programmatically Interpretable RL

@article{hughes1989functional,
  title={Why functional programming matters},
  author={Hughes, John},
  journal={The computer journal},
  volume={32},
  number={2},
  pages={98--107},
  year={1989},
  url={https://academic.oup.com/comjnl/article/32/2/98/543535},
  publisher={Oxford University Press}
}

@book{koza1994genetic,
  title={Genetic programming II},
  author={Koza, John R and
          others},
  volume={17},
  year={1994},
  url={https://amazon.com/s?k=9780262111706},
  publisher={MIT press Cambridge}
}

@article{allamanis2017learning,
  title={Learning to represent programs with graphs},
  author={Allamanis, Miltiadis and
          Brockschmidt, Marc and
          Khademi, Mahmoud},
  journal={arXiv preprint arXiv:1711.00740},
  url={https://arxiv.org/abs/1711.00740},
  year={2017}
}

@article{brockschmidt2018generative,
  title={Generative code modeling with graphs},
  author={Brockschmidt, Marc and
          Allamanis, Miltiadis and
          Gaunt, Alexander L and
          Polozov, Oleksandr},
  journal={arXiv preprint arXiv:1805.08490},
  url={https://arxiv.org/abs/1805.08490},
  year={2018}
}

@inproceedings{looks2005learning,
  title={Learning computer programs with the bayesian optimization algorithm},
  author={Looks, Moshe and
          Goertzel, Ben and
          Pennachin, Cassio},
  booktitle={Proceedings of the 7th annual conference on Genetic and evolutionary computation},
  pages={747--748},
  doi={10.1145/1068009.1068134},
  url={https://dl.acm.org/doi/abs/10.1145/1068009.1068134},
  year={2005}
}

@article{verma2018programmatically,
  title={Programmatically interpretable reinforcement learning},
  author={Verma, Abhinav and
          Murali, Vijayaraghavan and
          Singh, Rishabh and
          Kohli, Pushmeet and
          Chaudhuri, Swarat},
  journal={arXiv preprint arXiv:1804.02477},
  url={https://arxiv.org/abs/1804.02477},
  year={2018}
}

@article{akiba2013calibrating,
  title={Calibrating research in program synthesis using 72,000 hours of programmer time},
  author={Akiba, Takuya and
          Imajo, Kentaro and
          Iwami, Hiroaki and
          Iwata, Yoichi and
          Kataoka, Toshiki and
          Takahashi, Naohiro and
          Moskal, Micha{\l} and
          Swamy, Nikhil},
  journal={MSR, Redmond, WA, USA, Tech. Rep},
  url={https://pdfs.semanticscholar.org/1cde/a6fa2f0a400f509aed98f9a857ab1788257e.pdf},
  year={2013}
}

@book{alur2013syntax,
  title={Syntax-guided synthesis},
  author={Alur, Rajeev and
          Bodik, Rastislav and
          Juniwal, Garvit and
          Martin, Milo MK and
          Raghothaman, Mukund and
          Seshia, Sanjit A and
          Singh, Rishabh and
          Solar-Lezama, Armando and
          Torlak, Emina and
          Udupa, Abhishek},
  year={2013},
  doi={10.1109/FMCAD.2013.6679385},
  url={https://ieeexplore.ieee.org/abstract/document/6679385},
  publisher={IEEE}
}

@article{alur2016sygus,
  title={Sygus-comp 2016: results and analysis},
  author={Alur, Rajeev and
          Fisman, Dana and
          Singh, Rishabh and
          Solar-Lezama, Armando},
  journal={arXiv preprint arXiv:1611.07627},
  url={https://arxiv.org/abs/1611.07627},
  year={2016}
}

@inproceedings{singh2015predicting,
  title={Predicting a correct program in programming by example},
  author={Singh, Rishabh and
          Gulwani, Sumit},
  booktitle={International Conference on Computer Aided Verification},
  pages={398--414},
  year={2015},
  doi={10.1007/978-3-319-21690-4_23},
  url={https://link.springer.com/chapter/10.1007/978-3-319-21690-4_23},
  organization={Springer}
}

@book{solar2008program,
  title={Program synthesis by sketching},
  author={Solar-Lezama, Armando and
          Bodik, Rastislav},
  year={2008},
  doi={10.5555/1714168},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.9048&rep=rep1&type=pdf},
  publisher={Citeseer}
}

@article{feng2018program,
  title={Program synthesis using conflict-driven learning},
  author={Feng, Yu and
          Martins, Ruben and
          Bastani, Osbert and
          Dillig, Isil},
  journal={ACM SIGPLAN Notices},
  volume={53},
  number={4},
  pages={420--435},
  year={2018},
  doi={10.1145/3296979.3192382},
  url={https://dl.acm.org/doi/abs/10.1145/3296979.3192382},
  publisher={ACM New York, NY, USA}
}

@article{prose,
  title={Automating string processing in spreadsheets using input-output examples},
  author={Gulwani, Sumit},
  journal={ACM Sigplan Notices},
  volume={46},
  number={1},
  pages={317--330},
  year={2011},
  doi={10.1145/1925844.1926423},
  url={https://dl.acm.org/doi/abs/10.1145/1925844.1926423},
  publisher={ACM New York, NY, USA}
}
% see notes on prose framework up at kalyan2018neural

@article{neuralgpu,
  title={Neural gpus learn algorithms},
  author={Kaiser, {\L}ukasz and
          Sutskever, Ilya},
  journal={arXiv preprint arXiv:1511.08228},
  url={https://arxiv.org/abs/1511.08228},
  year={2015}
}

@article{nmt,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and
          Cho, Kyunghyun and
          Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  url={https://arxiv.org/abs/1409.0473},
  year={2014}
}

@inproceedings{ptrnets,
  title={Pointer networks},
  author={Vinyals, Oriol and
          Fortunato, Meire and
          Jaitly, Navdeep},
  booktitle={Advances in neural information processing systems},
  pages={2692--2700},
  url={http://papers.nips.cc/paper/5866-pointer-networks},
  year={2015}
}

@article{structuredattention,
  title={Structured attention networks},
  author={Kim, Yoon and
          Denton, Carl and
          Hoang, Luong and
          Rush, Alexander M},
  journal={arXiv preprint arXiv:1702.00887},
  url={https://arxiv.org/abs/1702.00887},
  year={2017}
}

@article{ntm,
  title={Neural turing machines},
  author={Graves, Alex and
          Wayne, Greg and
          Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  url={https://arxiv.org/abs/1410.5401},
  year={2014}
}

@article{neuralram,
  title={Neural random-access machines},
  author={Kurach, Karol and
          Andrychowicz, Marcin and
          Sutskever, Ilya},
  journal={arXiv preprint arXiv:1511.06392},
  url={https://arxiv.org/abs/1511.06392},
  year={2015}
}

@article{neuralprogrammer,
  title={Neural programmer: Inducing latent programs with gradient descent},
  author={Neelakantan, Arvind and
          Le, Quoc V and
          Sutskever, Ilya},
  journal={arXiv preprint arXiv:1511.04834},
  url={https://arxiv.org/abs/1511.04834},
  year={2015}
}

@article{hierarchicalmemory,
  title={Learning efficient algorithms with hierarchical attentive memory},
  author={Andrychowicz, Marcin and
          Kurach, Karol},
  journal={arXiv preprint arXiv:1602.03218},
  url={https://arxiv.org/abs/1602.03218},
  year={2016}
}

@article{npl,
  title={Neural program lattices},
  author={Li, Chengtao and
          Tarlow, Daniel and
          Gaunt, Alexander L and
          Brockschmidt, Marc and
          Kushman, Nate},
  url={https://openreview.net/forum?id=HJjiFK5gx},
  year={2016}
}

@article{cai2017making,
  title={Making neural programming architectures generalize via recursion},
  author={Cai, Jonathon and
          Shin, Richard and
          Song, Dawn},
  journal={arXiv preprint arXiv:1704.06611},
  url={https://arxiv.org/abs/1704.06611},
  year={2017}
}
