\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx, color}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{hyperref}
\usepackage[numbers]{natbib}
\usepackage{subfiles}
\usepackage{refcheck}
\usepackage{csquotes}
\usepackage{titlesec}

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\begin{document}

\subfile{title-page-ai.tex}

\tableofcontents

\pagebreak

\section{Research Direction} % \label{sec:research-direction}

\subsection{Program Synthesis}

\begin{displayquote}
    truly solving program synthesis is the last programming problem mankind will have to solve~\citep{nps}
\end{displayquote}

\begin{displayquote}
    If artificial intelligence (AI) is software 2.0~\citep{software20},
    then \emph{program synthesis} is software 2.0 applied to the field of software development itself.
\end{displayquote}
% I think only few will feel offended by the implication all program synthesis uses AI.

% TODO: talk more about the gaps between 1.0/engineering vs 2.0/AI, and how the present work aims to bridge those.

% synthesis
\emph{Program synthesis}~\citep{church1957applications} is the task to automatically construct a program%
~\footnote{While the definition of a program might be debatable as well, and there has been work on predicting type signatures from function names~\citep{wang2018predicting}, for the purpose of our paper we will focus on traditional executable programs, rather than e.g. 'programs' describing types.}%
that satisfies a given high-level specification,
be it a formal specification, a natural language description, full program \emph{traces}, input-output examples, or an existing program.~\citep{gulwani2017program}.
% ~\footnote{\url{https://en.wikipedia.org/wiki/Program_synthesis}}

This enables us to distill our modeled function
to a simplified discrete form that may well be intelligible to humans as well,
opening up opportunities for human-machine cooperation in writing software.
Specifically, this will allow machines to improve on programs written by humans, and the other way around.
As such, program synthesis may bring \emph{hybrid intelligence} to the field of software development.

And in fact, we may already see this in the proliferation of intelligent code completion tools%
~\footnote{These include Microsoft's \emph{Intellisense}, Google's \emph{ML Complete}, Jetbrains's code completion in their IntelliJ IDE, as well as Codota's \emph{TabNine}.},
as well as in a novel development paradigm in functional programming based on typed holes called \emph{type-driven development}~\citep{brady2017type}.

\subsection{Related fields}

To give more context on how program synthesis fits into the bigger picture,
we will briefly compare it to some other fields: program induction, supervised learning, as well as constraint satisfaction and discrete optimization.

\subsubsection{Program Induction}

Unfortunately the field suffers from competing definitions, blurring the distinction between what constitutes program \emph{synthesis} versus what constitutes program \emph{induction}. In short though, those in either field claim to be more general than the other branch.

The field of \emph{inductive programming}~\citep{popplestone1969experiment,plotkin1970note,fogel1966intelligent}, primarily known for its sub-branch of \emph{inductive logic programming}~\citep{muggleton1991inductive} focused on propositional logic, is simply the use of inductive logic for the purpose of automatic synthesis of programs, and was coined to distinguish itself from the \emph{deductive} techniques used in \citet{church1957applications}'s synthesis of circuits. Under this definition, the term program synthesis is used to refer to its original scope of program generation using deductive techniques.

To explain, whereas in the original problem definition the desired behavior was fully specified, program \emph{induction} aimed to generalize the problem to also tackle automatic generation of programs for which the desired behavior had only partially been made explicit.

Under this definition, there is no significant distinction between our present work and program induction's sub-branch of \emph{inductive functional programming}, focused on the generation of programs in functional programming languages such as Lisp~\citep{lisp} or Haskell.

Nevertheless, in current parlance \emph{program synthesis} is often used in a broader scope, extending from the original deductive approach to include inductive approaches as well. In this view, the two fields are distinguished in that program \emph{synthesis} is defined as to explicitly return a program, whereas program \emph{induction} learns to \emph{mimic} it rather than explicitly return it.~\citep{devlin2017robustfill,gulwani2017program,nps}
While this appears to clash with the latter term as used in the field of inductive \emph{functional} programming, this view likely stems from widespread use of the term in the field of inductive \emph{logic} programming.

This terminology itself is not of much concern for our present paper, as the boundaries between the fields have often been muddy.
Moreover, recent applications of AI to this field have led to the more recent term of \emph{neural program synthesis}~\citep{nps}. Therefore, we will simply settle for using 'program synthesis' to the field in general as well.

\subsubsection{Supervised Learning}

The above definition of program synthesis as explicitly returning a program is helpful to explain how it differs from \emph{supervised learning},
the machine learning task of learning a function that maps an input to an output based on example input-output pairs~\citep{russell2002artificial}.

Neural learning methods may be applied to any branch of program synthesis,
and several of these may in fact be tackled using setups involving supervised learning.
Of particular note here however is a branch referred to as \emph{programming by example} (PBE),
like supervised learning based on modeling the logic of given input-output pairs.

What sets these apart is that PBE fits a model into the discrete form of a given grammar to produce a program,
forcing one to instantiate such a model from probabilistic data interpretations,
whereas in supervised learning,
such probabilistic interpretations of the data may be taken at prediction time.

This also explains the relative benefits of these two fields.

Whereas supervised learning may stay in the continuous realm,
characterized by differentiable models optimizable by backpropagation~\citep{backproprnn},
and not limited in expressivity by the limitations of any particular grammar or set of operations.
This makes it well-positioned to solve problems deemed too complex for traditional programming, such as image recognition.

PBE may instead construct a traditional program,
which may be both deterministic as well as potentially faster to execute than predictions using the original machine learning model.
Moreover, using programs as a common denominator between human and machine-based programmers makes for human-intelligible machine-made models,
relevant in the field of \emph{interpretable} or \emph{explainable artificial intelligence}
~\footnote{
    One may note that if the goal in using program synthesis is to make models more interpretable,
    one could potentially start out by training a neural model, then approximate this by synthesizing a program similar to it. And in fact, \citet{pirl} apply exactly this approach for reinforcement learning.
},
while also enabling human-machine cooperation in the production and maintenance of software.

Furthermore, program synthesis may help tackle the problem in AI of \emph{composability}:
programs in their essence \emph{compose} operations into larger logical building blocks,
to be reused or further generalized.
In supervised learning, on the other hand,
one would traditionally need to retrain their model in the event a new predictor class is added,
which depending on the model complexity may be quite costly.
This challenge of composabability in AI has also been studied by the field of \emph{hierarchical reinforcement learning}~\citep{hierarchicalrl}.

In program synthesis, one may take an existing program, and synthesize variants intended to generalize the existing logic to match the new data.~\citep{myth}
This makes program synthesis well-suited to facilitate the automation of programming.
~\footnote{
    One may note that this would technically enable the synthesis of programs implementing machine learning models as well.
    However, such an approach would make for a relatively expensive evaluation function,
    and as such is traditionally delegated to the field of \emph{neural architecture search}.
}

In other words, whereas supervised learning makes for simpler learning,
program synthesis may make for programs that are more efficient, composable,
and providing better interaction with humans in terms of being understandable and incorporating knowledge of human experts.
% TODO: citation needed?
% table of relative merits: supervised learning side, pros: able to model ill-defined logic, more suited for stochastic predictions

\subsubsection{Constraint satisfaction vs. discrete optimization}

% relation to https://en.wikipedia.org/wiki/Constraint_programming : that seems a subset associated with logic programming
% relation to SMT: that's first-order logic, synthesis is second-order
% relation to SAT: that's SMT without linear constraints, so even less expressive
While its definition may appear to frame program synthesis as a type of \emph{constraint satisfaction} problem (CSP),
where a program either does or does not satisfy the given specification,
% ~\footnote{https://en.wikipedia.org/wiki/Constraint_satisfaction_problem}
one could also opt to approach it as a \emph{discrete optimization} problem,
% relation to https://en.wikipedia.org/wiki/Combinatorial_optimization : that one assumes a finite set of objects? 
as specifications such as input-output examples allow us to count the examples our candidate program satisfies.

Intuitively, a program satisfying part of our examples may be regarded as closer to a solution than one that does not satisfy as many.
Furthermore, additional considerations such as performance may further push us to find a solution that not only calculates outputs correctly but also runs within reasonable time or memory constraints.
These then provide a quantifiable feedback measure for us to optimize.

However, constraint satisfaction and discrete optimization were intended to solve fully specified problems (\emph{deduction}),
while in modern-day program synthesis, as we will explain later,
we usually need to settle for a \emph{partial} specification of the intended program's behavior (\emph{induction}).

As such, applying such techniques to the field of program synthesis using partial specifications may lead us to the problem of \emph{overfitting}:
while found solutions might well satisfy the \emph{specified} behavior,
the question would be whether these would also \emph{generalize} to match our \emph{intended} behavior,
as is the goal in program synthesis.

\subsection{Types of program synthesis}

While the common thread in program synthesis is that our intended output takes the form of a program,
sub-branches of this field are primarily defined by the types of input we use to come to this output.
Here we will give a brief overview of such variants of program synthesis.

\subsubsection{Synthesis from formal specifications}

Program synthesis was traditionally studied as a computer science problem,
where the problem was typically framed using a formal specification.
This problem was then tackled using e.g. an enumerative search, deductive methods, or constraint-solving techniques.~\citep{gulwani2017program}
% However, such formal specifications ended up about as hard to write as the original program,
% rendering this approach to the problem not very useful.

\subsubsection{Synthesis from natural-language descriptions}

Synthesis from \emph{natural-language} descriptions%
~\citep{neuralprogrammer,zhong2017seq2sql},
which might also involve \emph{speech recognition}%
~\citep{juang1991hidden,graves2013speech},
like PBE
% is an example of \emph{inductive} synthesis:
% as we are no longer tied to a formal specification,
lacks a formal specification,
as the language used is not necessarily constrained.
As such, this ends up as a supervised learning problem:
we take sample natural language inputs,
and try to learn their corresponding resulting programs.

This area is closely connected to the field of
\emph{personal virtual assistants}~\citep{virtualassistant,genie},
where response actions may be regarded as constrained synthesized programs,
although these might also feature unconstrained \emph{dialogue}~\citep{li2016deep} responses,
as well as \emph{goal-oriented} or \emph{task-oriented dialogue}~\citep{bordes2016goaldialogue,li2016taskdialogue}.

This task has also been combined with \emph{programming from examples}.~\citep{polosukhin2018neural}

\subsubsection{Program repair}

\emph{Program repair} is a synthesis task where the input to the synthesizer is a (broken) program that is to be fixed by making changes to the program.~\citep{demsky2006inference,kaleeswaran2014minthint}
This can be used in e.g. IDEs, to suggest users how they might potentially fix syntax errors or misspelled variables in their code.

As the input to this synthesis branch consists of existing programs,
it also has quite some overlap with \emph{static code analysis}~\citep{louridas2006static,brockschmidt2018generative},
which is about gaining insights into snippets of source code,
and as such may help provide features relevant for use in program repair.

\subsubsection{Source-to-source translation} \label{sec:source2source}

Similar to program repair, \emph{source-to-source translation}%
~\citep{loveman1977program,albrecht1980source,partsch1983program,waters1988program,visser2005survey,czarnecki2006feature,chen2018tree}
is a branch of synthesis where both the input and output are a program.
However, rather than fixing an existing program,
the task here is to learn a specific \emph{translation} task,
e.g. from one programming language to another.
Over recent years, this field has become an area of application for \emph{neural machine translation}.%
~\citep{cho2014nmt,kim2019translating}

\subsubsection{Programming by Example (PBE)}

Instead, it was found that for users, input-output examples were a more attractive way to specify desired program behavior,
although as an incomplete specification this made for a harder problem from the perspective of the synthesizer.~\citep{bodik2013algorithmic}
This field is named \emph{Programming by Example} (PBE).

As the specification is incomplete here, PBE is considered \emph{inductive} synthesis, as opposed to the \emph{deductive} synthesis where we do have a complete specification.
In other words, from the perspective of the synthesizer, PBE is generally a more difficult problem.

PBE may be further split up according to the type of program to be synthesized~\citep{bodik2013algorithmic},
generating logic programs (assigning truth values to variables),
or generating functional programs (e.g. Lisp, Haskell).

PBE too has branches based on deductive techniques (including \emph{type-theoretic} PBE),
inspired by synthesis from formal specifications.

% Note that these categories are not necessarily mutually exclusive:
% a synthesizer might for example have a natural language input,
% while using this to say generate a functional program.
% Either way,
Our work will focus on PBE in the category of functional programs,
where the goal is to automate traditional programming tasks.

\subsubsection{Synthesis from traces}

Synthesis from program \emph{traces}~\citep{koskimies1994automatic},
and the related synthesis from \emph{Linear Temporal Logic} (LTL)
specifications~\citep{camacho2019towards}, are about
a system reacting to sequences of inputs to mimic the desired program behavior.
These are useful for e.g. specifying the expected behavior of user interfaces.
Essentially this task may be viewed as a generalized version of PBE,
adding the additional challenge of figuring out which inputs triggered which state changes.

\subsubsection{Neural program synthesis (NPS)}

Over recent years, synthesis problems have been explored using \emph{machine learning} approaches,
under the name of \emph{neural program synthesis} (NPS)~\citep{nps}.
This area is about learning to solve a class of synthesis problems given data.
Note that this is a synthesis \emph{technique}, whereas the the above categories are synthesis \emph{problem types}.

Whereas traditional approaches in program synthesis (and particularly PBE) focused on constraining the large discrete search space,
such as deductive and constraint-solving approaches,
\emph{neural} program synthesis instead tends break down the problem by incrementally generating programs,
using continuous representations of the state space to predict the next token.
% be it in a sequential fashion~\citep{npi,neuralmachinetranslation,alphanpi},
% or in a structured one based on abstract syntax trees (ASTs)~\citep{nsps}.

\subsection{Challenges}

\subsubsection{Challenges of program synthesis}

While considered a holy grail of computer science~\citep{gulwani2017program},
program synthesis in general is a challenging task, characterized by large search spaces,
e.g. a space of $10^{5943}$ programs discovering an expert implementation of the MD5 hash function.~\cite{gulwani2017program}

% \subsubsection{Challenges of programming by example}

\subsubsection{Challenges of type-theoretic PBE}

% \subsubsection{Challenges of deductive synthesis}

% Deductive synthesis includes both synthesis from formal specifications as well as deductive techniques used in PBE, which includes type-theoretic approaches.

One issue with type-theoretic approaches to PBE is that,
while accurate specifications of program behavior could be written through the
use of e.g. refinement types~\citep{synquid} or succinct types~\citep{guospeeding},
types of a level that users may be more familiar with,
which may include polymorphic types or even algebraic data types,
may end up not sufficiently expressive: while certainly constraining the program space,
examples may still be needed to disambiguate between potential candidate programs.

\subsubsection{Challenges of neural PBE}

While the typically incremental approach of neural program synthesis breaks down the problem of PBE by considering one part of the program at a time, the original challenge remains:
our search space typically remains large, meaning it will not be viable to proportionally scale our training sets by program size.

Whereas a program synthesizer may be programmed or taught to output programs adhering to a given grammar,
we may generally only be able to evaluate the quality of \emph{complete} programs:
there is typically no guarantee that \emph{partial} constructions of the full program would \emph{also} qualify as a full executable program adherent to the grammar.
As a result, neural synthesizers will have little intermediary feedback to go by, limiting their effectiveness.

But if only \emph{complete} programs can be evaluated for validity and behavior, then 
we will be ill-equipped to provide synthesizers with an accurate understanding of partial programs,
which make up for a large part of our prediction steps.
As such, it would be desirable to somehow supervise the intermediate prediction steps.

\paragraph{Challenges of sequential-based neural PBE}

Most neural synthesis techniques, particular those using a \emph{sequence-to-sequence} approach~\footnote{
    While in tree-based synthesis techniques it might not be possible to \emph{execute} programs either,
    this should at least result in an (incomplete) AST, which should be significantly easier to learn to embed given the knowledge of how to embed a complete AST than a program that does not even parse.
    % TODO: confirm if there is a distinction between tree-based / node-based
},
additionally face the issue of dissonance between their representation of complete programs and that of intermediate states.
As such intermediate states do not in general constitute valid programs,
these neural synthesizers have an additional task to solve:
compensating for their lack of an inherently meaningful incremental state.

\subsection{Research question}

\subsubsection{Complementary strengths}

Our key observation here is thus that input-output examples and types are quite complementary as specifications constraining our program behavior:
whereas input-output examples are relatively expressive, they may only help us to evaluate the quality of complete programs;
types, on the other hand, are by themselves not usually descriptive enough of our task,
but may help us to evaluate and inform further incremental synthesis steps even of incomplete functions still containing holes.

\subsubsection{Hypotheses}

We therefore hypothesize that program synthesizers may thus capitalize on this synergy by utilizing both types of information,
rather than settling for only one of the two, as most existing methods have done.%
~\footnote{
    While one might wonder if this constrains our idea to the subset of PBE problems where type information is available,
    this limitation is essentially meaningless: when one has input-output examples, one should generally be able to infer their type.
    This would render our idea applicable for practically any (neural) methods for PBE.
}%
~\footnote{
    Whereas one might wonder if a focus on types may constrain our method to synthesizing statically typed languages,
    in the functional paradigm in particular, we believe there are workarounds to this.
    While we certainly believe users may find switching to a statically typed functional language to be beneficial~\citep{hughes1989functional},
    an alternative would be to (1) have one algorithm translate the partial program to a statically typed language,
    (2) complete the program using program synthesis from there,
    then finally, (3) using a third algorithm, translate the full program back to the target language.
}

Specifically, we hypothesize that the effectiveness of neural program synthesis may be improved by
adding type information as additional features during training and testing,
which in AST-based neural program synthesis may help supervise each incremental prediction step.

\begin{displayquote}
    \emph{Research question \#1: can neural program synthesis methods benefit from using types as additional features?}
\end{displayquote}

Furthermore, in an incremental node-based synthesis approach, we hypothesize that after each synthesis step,
in languages featuring typed holes we may pre-compile even partial programs such as to provide the synthesizer with immediate feedback.

\begin{displayquote}
    \emph{Research question \#2: can neural program synthesis methods benefit from using compilation checks as additional features?}
\end{displayquote}

% An alternative approach would be to enumerate any possible options for a given node,
% then filtering this list of options based on such compiler feedback.%
% ~\footnote{One potential concern here is the scalability of such enumeration as the number of valid options grows.}

% This is indeed done for type-based program synthesis~\citep{myth},
% and we hypothesize neural synthesis approaches may benefit from such pre-filtering as well,
% reducing synthesis to a ranking problem of (partial) candidate programs.

% \begin{displayquote}
%     \emph{Research question \#3: can neural program synthesis methods benefit from type-based filters by reframing the problem to learning-to-rank?}
% \end{displayquote}

% Lastly, we hypothesize that these added features still offer added value over deduction-based approaches aiming to push input-output examples through the grammar (investigate separately for datasets without and with many functions that are not invertible) and using type-based filters:

% \begin{displayquote}
%     \emph{Research question \#4: can deduction-based neural program synthesis methods benefit from using types and compilation checks as additional features?}
% \end{displayquote}

\pagebreak

\section{Expected Contribution} % \label{sec:expected-contribution}

The present work aims to be the first experiment to:
\begin{itemize}
    \item bring the \emph{type-based information} traditionally used in program synthesis into the newer branch of neural program synthesis;
    \item \emph{bridge} the two fields, such as to find a best-of-both-worlds golden mean;
    \item use this type info to better \emph{constrain the search space}, improving the effectiveness of (neural) program synthesis methods;
    \item show that the synthesis of statically typeable programs may benefit from techniques \emph{specific} to this domain, and therefore for the purpose of automatic programming merits further study in itself;
    \item offer an \emph{open-source implementation} of the algorithm described in \citet{nsps};
    \item generate a \emph{dataset} for neural synthesis of functional programs;
    \item lay out a way of \emph{generating} such datasets, including an open-source implementation, addressing the current reliance on hand-crafted curricula~\citep{nps};
    % provide an implementation that may be one of the first machine learning projects utilizing
    \item showcase \emph{static tensor typing} as offered by the \emph{HaskTorch} deep learning library~\citep{hasktorch}.
\end{itemize}

\pagebreak

\section{Existing approaches to PBE}

PBE has traditionally known heuristics such as
\emph{Version Space Algebras} (VSAs), which aim to constrain
grammar productions with a limited number of examples.

Another useful tool is \emph{ambiguity resolution},
i.e. requesting user input to resolve ambiguity
in the event that multiple candidate programs
fulfill the given input-output example pairs.%
~\citep{gulwani2017program}
While we will come back to ambiguity resolution as it is
alleviated by \emph{active learning},
these two techniques are primarily used to complement
other methods we will introduce here now.

Please do note that program synthesis has been somewhat different
from other branches of machine learning, such as image recognition:
unfortunately the field has been so diverse that there has been little
standardization of benchmarking tasks to compare approaches like
\emph{ImageNet}~\citep{deng2009imagenet} had done for computer vision tasks.
While this means we will not present statistics
comparing the performance of these various approaches,
we will instead lay out their conceptual differences and weaknesses.

% \subsection{Search-based PBE}

\subsection{Enumerative search}

The naive approach to synthesis would be to enumerate all the possible programs in our search space,
and for each one evaluate whether it satisfies our task specification.
This is called \emph{enumerative} or \emph{depth-first search} (DFS).
As one might expect, such an approach does not generalize \emph{scale} well with search space size however.

\subsection{Evolutionary algorithms}

\emph{Evolutionary algorithms}~\citep{eiben2003introduction} are a black-box optimization method of potentially discrete functions.

\emph{Genetic programming} is a branch of evolutionary algorithms based on \emph{tree} representations,
rendering it viable to \emph{evolve} programs using some AST representation.~\citep{koza1994genetic}
Such \emph{evolution} is a search method that consists of generating programs similar to existing promising candidates, using either \emph{mutations} or \emph{combinations} of existing candidate solutions.

However, while such heuristics help improve on the baseline of enumerative search,
unfortunately such optimization is a one-shot exercise:
it has little notion of \emph{learning} from previous problems to improve performance on future similar problems.%
~\footnote{
    Technically one way to let an evolutionary algorithm \emph{learn} would be to seed runs on new problems using solutions from previous similar problems.
    However, such choices would be left up to the user,
    and are not something that the evolutionary algorithm itself is able to help out on.
    In practice, even under the same class of problems,
    different problem instances will likely require different end solutions,
    largely rendering this hack moot.
}
As such, evolutionary approaches have been more popular in areas where their strategy of \emph{local search} would pose an advantage, such as for \emph{super-optimization}~\citep{schkufza2016stochastic} (optimizing the performance of an existing program) and \emph{program repair}~\citep{weimer2009automatically,forrest2009genetic}.

\subsection{Active learning and Bayesian optimization}

\emph{Active learning}~\citep{settles2009active} and
\emph{Bayesian optimization}~\citep{mockus2012bayesian}
are two techniques intended to minimize a required number of
expensive evaluation rounds in modeling discrete functions.
Their difference however lies in their intended objective:
the objective in \emph{active learning} is to most accurately
model the underlying function, while in
\emph{Bayesian optimization} the objective is to maximize
a given reward (as a total over the number of evaluations),
which renders it a trade-off of \emph{exploration} versus
\emph{exploitation} similar to that of reinforcement learning.

Within program synthesis, active learning is primarily used to
facilitate effectively requesting interactive user feedback when
trying to synthesize a particular function in PBE when faced
with ambiguity of user intent.~\citep{shen2019using}%
~\footnote{
    Interactive user feedback is a useful technique in the face of ambiguity,
    but for the purpose of this paper we will not focus on this technique.
}

Unlike evolutionary algorithms, these techniques model the underlying function,
allowing one to generalize to other problem instances as well.
As a result, we can use these not only in interactive feedback,
but also to directly decide which programs we wish to evaluate programmatically.
However, programmatic evaluations are typically cheap,
making this use-case less of a fit given the premise of these techniques.

While Bayesian optimization has been used to this end~\citep{looks2005learning},
the problem with using it this way is that the cost complexity of this technique
scales terribly to larger search spaces, which are common in program synthesis.
The risk with this is that this may make it much faster to just evaluate
multiple programs in the search space than it would be to spend those CPU cycles
figuring out which single evaluation might be most promising or informative.
As such, subsequent attempts have limited its use to constrained
sub-problems like \emph{sketch filling}.~\citep{verma2018programmatically}

\subsection{Oracle-guided synthesis}

One attempt to overcome the computational complexity of program synthesis
has been \emph{oracle-guided synthesis}~\citep{solar2008program},
which splits the synthesis task into generating and filling of program
\emph{sketches}~\citep{murali2017neural}.
Unlike full synthesis itself, sketch filling is not a second-order but a
first-order logic problem, enabling the use of constraint solving methods
such as satisfiability (SAT) or satisfiability modulo theories (SMT) solvers%
~\citep{akiba2013calibrating,alur2013syntax,alur2016sygus,rosette,architecture}%
~\footnote{
    SMT solvers combine SAT-style search with theories like arithmetic and inequalities.
} for the filling of sketches, potentially further extended with
\emph{conflict-driven learning}~\citep{feng2018program,hornclauses},
which help \emph{backtrack} if the branch explored turns out unviable.
The point here is that if a given sketch has multiple holes,
once a filled version turns out unviable due to a certain production rules used for one of its holes,
other variants involving the faulty choice in question may be ruled out as well.

However, like evolutionary algorithms, this is a one-shot technique
that does not learn across problem instances.
As such, it is likely not as efficient if our goal is to create a general synthesizer,
although it could still be incorporated into hybrid approaches~\citep{deepcoder}.

\subsection{Deductive techniques}

\emph{Deductive} search techniques for PBE were inspired by such
techniques used in synthesis from formal specifications,
but have been applied to the \emph{inductive} task of PBE as well.
Deductive techniques are based on theorem provers,
and recursively reduce the synthesis problem into sub-problems,
propagating constraints.
These include approaches based on \emph{inverse semantics} of
DSL operators and \emph{type-theoretic PBE}.

\subsubsection{Inverse semantics}

The idea of \emph{inverse semantics} is to reduce the complexity
of the synthesis task by using inverse logic.~\citep{flashmeta,prose}
This is a top-down search where we would take a grammatical
production rule, presume it to be our outer expression,
and use its inverse logic to propagate our original
input-output examples to its sub-expressions.
This way we have obtained a simpler sub-problem to solve.

The generalization of such a function inverse to work with functions
taking multiple parameters using \emph{witness functions},
each representing a function inverse for a given parameter.

While these are a useful search technique however, their
use is unfortunately limited to invertible operations,
rendering this a helpful complement to, yet not a
reliable alternative to other PBE methods.%
~\footnote{
    A recent potential workaround not reliant on invertability
    has been the approach by \citet{odena2020learning},
    who would, given properties of a function composition
    $f \circ g$ and of $f$, use machine learning to predict
    the properties of $g$.
    However, it is not immediately clear if this technique
    has a straight-forward equivalent in the domain of
    input-output examples.
}

\subsubsection{Type-theoretic PBE}

\emph{Type-theoretic} deductive search is about the use of programming
types to constrain the synthesis search space.
While there have been pure type-based approaches based on e.g.
refinement types~\citep{synquid} or succinct types~\citep{guospeeding},
which can be used as a powerful way of specifying program behavior,
unfortunately these typically end up in a similar pitfall as
synthesis from formal specifications;
they often require the user to essentially write a specification
that may end up similar in complexity to the actual program itself,
in a sense potentially defeating the point of using a synthesizer.

% As such, as with synthesis from formal specifications itself,
% it may be more realistic to use type-based logic from
% the type information we do have readily available,
% using this to complement other synthesis methods,
% rather than presume we may fully rely on just type-theoretic PBE.

However, this branch is nevertheless useful in combination with other methods,
and the use of type-theoretic deductive search has been combined with PBE by \citet{myth}.
While this direction seems quite promising,
it unfortunately suffers from similar issues as other non-neural search methods:
while it may work as a one-shot technique,
there is no sense of learning across multiple problem instances.
% TODO: confirm if true. this presumes there is stuff to learn that these proof-theoretic techniques didn't have hard-coded already. do they really *know* *everything* in terms of function behavior patterns that could be learned tho?

% \subsection{Learning-based PBE}

\subsection{Neural program synthesis (NPS)}

More recently, PBE has been explored using machine learning approaches, under the name of \emph{neural program synthesis} (NPS)~\citep{nps}.
Whereas traditional approaches in program synthesis (and particularly PBE) focused on constraining the large discrete search space,
such as deductive and constraint-solving approaches,
\emph{neural} program synthesis instead tends to incrementally generate programs,
using continuous representations of the state space to predict the next token,
be it in a sequential fashion~\citep{npi,neuralmachinetranslation,alphanpi},
or in a structured one based on abstract syntax trees (ASTs)~\citep{nsps}.

Unfortunately though, program synthesis in its general sense has been less straight-forward to tackle by neural methods than other AI problems,
as our search space is typically discrete, meaning we cannot simply apply gradient-based optimization such as \emph{stochastic gradient descent} (SGD).~\citep{nps}

This issue can be worked around in different ways:
\begin{itemize}
    \item Using a \emph{differentiable interpreter} to directly enable gradient-based optimization.~\citep{forth,terpret,houdini,feser2016differentiable,rocktaschel2017end}
          However, such purely SGD-based methods have proven less effective than traditional or mixed methods.~\citep{terpret}    
    \item Using \emph{strong supervision}, i.e. create a differentiable loss signal
        to supervise synthesis training by checking if the synthesized program is \emph{identical} to the target program,
        rather than if it has \emph{equivalent behavior}.
        This approach unfortunately simplifies our problem \emph{too much}%
        ~\footnote{
            In reality, we wish to condition our model to synthesize not just the known programs,
            but to generalize to learn to synthesize \emph{unknown} programs matching our task specification as well.
            Supervising by a given 'known correct' program instead tells our model that other programs matching our specification somehow do not qualify as correct.
            As a result, such supervision requires that the training dataset provides a representative sample of our full program space.
            Note that this assumption is broken for datasets much smaller than the program space,
            meaning that this approach does not scale well to bigger search spaces.
            % TODO: add references?
        }, but does make for a relatively simple setup.
    \item Using \emph{weak supervision}~\citep{mapo},
        which tends to address the problem of reward differentiability by using \emph{reinforcement learning} techniques to estimate a gradient to optimize by%
        ~\citep{chen2017towards,bunel2018leveraging,xu2019neural,camacho2019towards},
        so as to learn to synthesize by \emph{trying} based on program performance rather than from direct supervision signals.
        This approach requires a more complex setup, but solves the issues of supervised neural synthesis.
    \item using neural methods in a \emph{hybrid} setup. This approach is explored further in section \ref{sec:ngs}.
\end{itemize}

\subsubsection{Graph-based NPS}

One neural network type used in NPS has been
\emph{graph neural networks}~\citep{shuman2013emerging,wu2020comprehensive}.
These have been used for representing programs~\citep{allamanis2017learning}
as well as for generative modeling of source code~\citep{brockschmidt2018generative}%
to better capture e.g. the relations between variable occurrences,
which \citet{brockschmidt2018generative} suggested would be of use in areas such as \emph{code repair},
where the input to the synthesizer is a (broken) program that is to be fixed.

Such methods could also reasonably be extended to facilitate training of embeddings for neural synthesizer using \emph{transfer learning}~\citep{pan2009survey} or \emph{multi-task learning}~\citep{multitasklearning} setups.
However, while these appear to be a potentially useful complement to other techniques
in the role of program encoder~\footnote{
    One caveat here is that graph neural networks' use in capturing
    variable relationships is only useful if one is synthesizing new variables,
    which in our present \emph{pointfree} DSL will remain out of scope,
    as we will explain in further detail later.
}, these appear not to have been designed with PBE in mind specifically.

\subsubsection{Information retrieval methods in NPS}

A different approach has been to apply techniques from the field of
\emph{information retrieval} (IR) such as \emph{learning-to-rank} (LTR) methods%
~\citep{singh2015predicting}, which aims to learn to judge the
relative quality of programs to rank a satisfactory program candidate on top,
rather than learning to assign scores for each individual program candidate.

However, this method was primarily intended for automated ambiguity resolution;
as one might notice,
it has a cost complexity linear to the number of candidates evaluated,
rendering it essentially the ambiguity resolution equivalent of enumerative search.

As such, this method is useful as a tiebreaker complementing existing methods,
as a potential alternative to other neural models within a top-down search,
or as the neural component in the \emph{neural-guided search} methods discussed below.%
~\footnote{Technically LTR could perform PBE by itself, but this would not scale well.}
While the insight of this approach in itself is valid,
it leaves unanswered questions regarding e.g. feature selection.

\subsubsection{Sequence-based NPS}

Neural synthesis methods typically employ \emph{sequence-to-sequence}
(or simply \emph{seq2seq}) techniques~\citep{npi,neuralmachinetranslation,alphanpi},
such as the \emph{recurrent neural network} (RNN)~\citep{backproprnn}
and \emph{long short-term memory} (LSTM)~\citep{lstm},
leveraging techniques commonly used in \emph{natural language processing} (NLP)
to represent program synthesis as a sequence prediction problem.%
~\footnote{
    Although many might be most familiar with \emph{seq2seq} methods for their prevalence in NLP,
    the original experiment for the RNN however did in fact test it
    on an inductive logic programming task!~\citep{backproprnn}
}

Such sequential neural synthesizers have been extended with mechanisms such as convolutional recurrence~\citep{neuralgpu}, attention~\citep{nmt,ptrnets,structuredattention}, memory~\citep{ntm,neuralram,neuralprogrammer,hierarchicalmemory}, function hierarchies~\citep{npi,npl}, and recursion~\citep{cai2017making}.

However, while a hypothetical synthesizer only producing compilable programs would always have direct feedback to its program embeddings,
this feedback signal is much delayed if a synthesizer would gradually synthesize a program e.g. one character at a time,
only learning about resulting program behavior once the program is complete.

As such, sequence-based neural techniques must learn quite a lot:
\emph{in addition to} (continuous logical equivalents of) the traditional compiler tasks of \emph{lexing} input into token categories,
\emph{parsing} these token sequences into hierarchical structures (ASTs),
and interpreting these to \emph{execute} them as programs,
these synthesizers must additionally learn how to construct and update a (memorized) state so as to ultimately,
when the synthesizer considers its code complete, obtain a correct program.

\subsubsection{Tree-based NPS}

Since then, there have also been approaches framing program synthesis by representing programs as \emph{abstract syntax trees} (ASTs) rather than as sequences~\citep{polosukhin2018neural},
allowing such methods to use \emph{tree-structured networks}%
%  such as \emph{recursive} neural networks
.
Of particular interest to us in this category has been the work of \citet{nsps},
which we will introduce in more detail in section \ref{sec:nsps}.

\subsubsection{Neuro-symbolic program synthesis (NSPS)} \label{sec:nsps}

The \emph{neuro-symbolic} program synthesis method introduced in \citet{nsps} improves on existing \emph{sequence-to-sequence}-based neural synthesis models by using a tree-based neural architecture they call the \emph{recursive-reverse-recursive neural network} (R3NN).
This aims to make predictions on credible rule expansions to fill holes in \emph{partial program trees} (\emph{PPTs}, basically ASTs containing holes) based on the program's content and structure,
while additionally conditioning on the input/output examples, as seen in figure \ref{nsps}.
The workings of the R3NN are illustrated in figure \ref{r3nn}.

It first makes a \emph{recursive pass} from the leaves through \emph{multi-layer perceptron}s (MLPs)~\citep{rosenblatt1961principles} at each branch,
all the way to the root node of the partial program, making for an embedding of the full program so far;
then a \emph{reverse recursive pass} from this root, again through MLPs, back to the leaves,
which now have their individual embeddings instilled with structural information about the entire program and how they fit into this larger structure.

To condition the partial program tree embedding on the input-output examples,
they try out different example encoders, starting out with a simple LSTM baseline but progressing through various versions of a \emph{cross-correlation encoder},
sliding the output and input feature blocks over one another to calculate their \emph{cross-correlation}~\citep{bracewell1986fourier},
then aggregating these by sum or concatenation,
% using LSTMs instead of dot products then summing,
then using two more involved approaches using LSTMs.

Of these, they found their aggregation by concatenation to only perform on par with their LSTM baseline (both 88\% accuracy on their test dataset) while the sum aggregation performed more poorly (65\%),
whereas their further additions only made for minor improvements on this (91\% each).
% TODO: stop being hand-wavy about the last encoder, I need to try and understand this better.
They additionally empirically test out different times at which to incorporate the input-output conditioning,
and found pre-conditioning (adding them before the recursive pass) to work best.

For the training phase they use the simple setup of supervising by the task function,
while for the test phase they sample 100 programs from their trained model,
considering the task function to have passed if any of these demonstrate the correct behavior on input/output.

\begin{figure*}
    \begin{tabular}{cc}
        \begin{minipage}{0.45\linewidth}
            \includegraphics[scale=0.16]{figures/tree2.png}
        \end{minipage}
        &
        \begin{minipage}{0.55\linewidth}
            \includegraphics[scale=0.16]{figures/tree3.png}
        \end{minipage}
        \\
        (a) Recursive pass & (b) Reverse-Recursive pass
    \end{tabular}
    \caption{(a) The initial recursive pass of the R3NN. (b) The reverse-recursive pass of the R3NN where the input is the output of the previous recursive pass.}
    \label{r3nn}
\end{figure*}

\begin{figure*}
    \begin{tabular}{c|c}
        \begin{minipage}{0.5\linewidth}
            \includegraphics[scale=0.3]{figures/nsps_training.pdf}
        \end{minipage}
        &
        \begin{minipage}{0.5\linewidth}
            \includegraphics[scale=0.3]{figures/nsps_test.pdf}
        \end{minipage}
        \\
        (a) Training Phase & (b) Test Phase
    \end{tabular}
    \caption{overview of the Neuro-Symbolic Program Synthesis model~\citep{nsps}}
    \label{nsps}
\end{figure*}

Some critiques of this model have included it being harder to \emph{batch} for larger programs due to its tree-based architectures,
as well as its pooling at the I/O encoding level being harder to reconcile with \emph{attention} than models using \emph{late pooling}.~\citep{devlin2017robustfill}

For our purposes, by merit of the Microsoft Excel \emph{FlashFill} domain this model was tested on,
it also shares a weakness with other neural synthesis models:
as these have usually been applied to \emph{untyped} domains,
they have not been augmented to use information on \emph{types},
while existing \emph{type-theoretic} synthesis approaches have shown this info to be highly valuable.

\subsubsection{Neural-guided search} \label{sec:ngs}

\emph{Neural-guided search} is an approach to \emph{hybrid} neural synthesis,
combining symbolic and statistical synthesis methods.~\citep{nps}
It employs neural components to indicate order within traditional search methods
such as enumerative search (possibly pruned using deduction),
'sort-and-add' enumeration or sketch filling.~\citep{deepcoder}

\citet{kalyan2018neural} built on this to extend the guidance to each search step,
integrating \emph{deductive} search (e.g. a SAT/SMT solver, extensions
like \emph{conflict-driven} learning~\citep{feng2018program}),
a \emph{statistical model} judging generalization,
and a controller deciding which of the model's suggested branches
to explore (branch-and-bound~\citep{kalyan2018neural}, beam search%
~\citep{polosukhin2018neural}, A\*~\citep{lee2018accelerating}).
The statistical model mentioned here is where other
neural synthesis methods would fit into this approach.
Also of importance here is deciding when to \emph{prune} branches,
although near the leaves this check may be slower
than to just explore any that remain.~\citep{polozov}

\citet{feng2018program} expanded on deductive techniques
like SMT solvers by \emph{conflict-driven} learning,
ensuring that if e.g. a \emph{map} operation would yield an
output list length not corresponding to the desired length,
other operations suffering from the same issue such as
\emph{reverse} and \emph{sort} would be automatically ruled out as well.

\citet{zhang2018leveraging} focus on incorporating deduced constraints
into the statistical model, to allow taking this info into account
in the decision of which branches to focus on.
Another similar effort has been that of \citet{odena2020learning},
which adds additional features describing properties of a function.
Types however have so far been missing here.

Compared to other neural methods, neural-guided search seems
more of a complementary than a competing effort.
The engineering involved to conciliate the benefits of
different approaches here may be quite involved, and as such
are likely less common in research papers comparing neural components,
their benefits in production systems seem clear.

\pagebreak

\section{Methodology} % \label{sec:methodology}

To explain our design decisions in this domain,
we will go by the synthesizer taxonomy of \citet{gulwani2017program},
in which a synthesizer is typically characterized by three key dimensions:
\begin{itemize}
    \item the kind of \emph{constraints} that it accepts as expression of \emph{user intent};
    \item the \emph{space} of programs over which it searches;
    \item the \emph{search technique} it employs.
\end{itemize}

\subsection{User intent}

User intent in program synthesis can be expressed in various forms, including logical specification~\citep{temporalstreamlogic} (among which \emph{types}~\citep{synquid}),
input-output examples, traces, natural language~\citep{abstractsyntaxnetworks},
partial programs, or even related programs.~\citep{gulwani2017program}

As the constraints to express user intent, we would like to use \emph{input-output examples},
which may be considered a compromise between what is easier for the \emph{end-user},
who may ideally prefer natural-language descriptions of program behavior,
versus what is easier for the \emph{synthesizer},
which may ideally prefer a complete formal specification of program behavior.

This puts us in the field of programming by example (PBE),
which has a broad area of application despite being conceptually simple%
~\footnote{
    Although arguably the potential applications of synthesis from natural-language descriptions are even broader,
    in terms of computational requirements and dataset generation it is a much harder problem.
}.

To (1) reduce ambiguity, (2) increase result quality, and (3) speed up synthesis, a synthesizer may be passed more information in various ways:
\begin{itemize}
    \item additional data within the same mode of user intent, e.g. further input-output examples;
    \item an additional expression of user intent of a different types, e.g. a natural language description~\citep{polosukhin2018neural} or type signature of the desired function~\citep{myth};
    \item more descriptive types~\citep{synquid};
    \item additional features describing properties of the function~\citep{odena2020learning}.
\end{itemize}

Of interest here is the realization here that, in modern programming languages, even without explicit type annotations may types be \emph{inferred}.
% TODO: implication: language preferably should have type inference
This is then a hidden benefit of synthesis from input-output examples:
if the types of input-output example pairs may be \emph{inferred}, then we may regard this as free additional information we can incorporate in our synthesis process.%
~\footnote{
    Optionally, it may be of interest to allow users to,
    in addition to the function types implied in their input-output examples,
    still let them explicitly clarify their desired function type manually as well.
    While on first glance this may seem potentially redundant with input-output pairs,
    it may actually help to constrain the potential function space to a sufficiently general form,
    benefiting both the user,
    by yielding a mode widely-applicable function,
    while also benefiting the synthesizer,
    by helping to constrain the search space.
}

\subsection{Program search space}

The program search space consists of the synthesis language (defined by a \emph{context-free grammar}),
either general-purpose or a \emph{domain-specific language} (DSL),
potentially further restricted to a subset of its original operators,
such as by providing a whitelist of \emph{operators}.

The trade-off here is one of expressiveness (can we write a satisfactory program, ideally using not too much code?) versus limiting our search space (ensure we can find a solution within too long).

One would then wonder: how might we achieve the highest amount of expressiveness within a limited search space?

Within this context, it would seem preferable to pick a limited grammar in the functional programming paradigm using static typing.

\subsubsection{Static typing}

Of our goals of expressiveness versus a limited search space,
\emph{types} would seem able to help us primarily in limiting our search space:
we would like them to prune out any parts of the search space that are not sensible.

As we have laid out above,
types are nearly a \emph{free lunch} when it comes to restricting the synthesis search space,
as per our hypothesis \#1 thereby improving synthesis performance.

\paragraph{Types versus grammar: generics}

However, one might note here that traditionally,
search spaces in program synthesis have been restricted not using types,
but using \emph{context-free grammars},
an approach that is the subject of the \emph{Syntax-Guided Synthesis competition} (SyGuS-Comp)~\citep{sygus}.

One might wonder: how then would restrictions based types compare to restrictions imposed by a grammar?

In a \emph{monomorphic} type system,
containing only simple (unparametrized) types such as \emph{boolean} or \emph{integer},
restraining a search using types is in fact \emph{equivalent} to a grammar where types are used as symbols in the grammar.
One might note that this equivalence does not imply that neither of these approaches is preferable:
it in fact means that a simpler and more generic grammar may achieve the same thing without losing expressivity,
allowing better reuse of logic.

However, although static typing has been around since the days of \emph{Fortran},
what makes the use of types different from,
and more powerful than grammars in restricting the search space,
is the use of \emph{type variables} and \emph{parametric polymorphism}:
a function \emph{append} may work using either lists of numbers or lists of strings.%
~\footnote{Left out of scope here are \emph{refinement types}, which further aid synthesis based on conditions.}
As such, its type signature may be made \emph{generic} such as to have its return type reflect the types of the parameters used.
Similarly, an \emph{identity} function could work on any different type, simply returning its input values.

Having such information available at the type level may add additional information over what is used in the simpler case above.
For example, a function to look up elements in a list based on their respective locations might take as its inputs one list containing any type of element, along with a second list of integers containing the indices.

Now, in a \emph{context-free} grammar,
such distinctions could not be expressed in a meaningful way:
while one might initially attempt to patch this by enumerating the different options for any symbolified parametric types in the grammar,
with the addition of new operations, types, parametric types, and higher-order parametric types,
such a grammar would quickly explode to the point of no longer remaining a reasonable abstraction to a human observer:
suddenly a hypothetical \emph{identity function} in the grammar, for one,
would need to be duplicated at the left-hand-side (LHS) level of the grammar for every known type in the grammar,
including for potential parametric types (list of booleans, list of integers, list of lists of booleans, \dots).

As such, one may regard the reliance of types over a grammar for the purpose of restricting the search space as a \emph{generalization} of solely relying on a \emph{grammar} for the purpose of restricting the search space.

Nevertheless, it should be noted here that not all programs benefit from this,
but only those for which potential building blocks (or \emph{operators}) do in fact include statically typed functions making use of either parametric return types, that is to say, return types containing type variables.

As a result, this restriction limits our method from adding value for direct synthesis of programming languages that are dynamically typed,
e.g. Ruby language, as well as those lacking parametric polymorphism, e.g. Go language.
However, this only partly hinders those wishing to synthesize programs in such languages:
rather than synthesizing in such languages directly, we would suggest synthesizing from the language that \emph{best constrains} the search space,
then using source-to-source translation methods as discussed in section \ref{sec:source2source} (e.g. neural machine translation~\citep{kalchbrenner2013recurrent}),
using the synthesized program as input to obtain a program translated into the target language.
% TODO: make into experiment hypothesis?

\paragraph{Types versus expressiveness}

However, in order to nevertheless achieve a high degree of expressiveness,
we simultaneously require a type system powerful enough to model any logic we might wish to utilize.

Now, in order to \emph{further} restrain our search space as much as possible,
this means we would generally prefer to have our functions be as generally applicable as possible.

So far the best attempts at generalizing functionality in programming languages has taken after the abstractions discovered in \emph{category theory}~\citep{eilenberg1945general,awodey2010category},
a branch of mathematics 
% priding itself in generality to the point where it has earned the moniker \emph{abstract nonsense},
formalizing mathematical structure,
which defines abstract categories based on their defining properties.

The implication of this is that functions often used on a list structure,
such as \emph{map} or \emph{traverse},
would no longer be defined to take \emph{list}s,
but to instead operate on the more abstract categories of \emph{functor} and \emph{traversable},
both of which would the \emph{list} data structure would be a \emph{member} of,
generalizing their area of applicability to extend to other data structures as well.

Such abstractions will require a type system supporting constructs such as \emph{type classes}~\citep{blott1991type}.
This requirement narrows down our potential choice for a synthesis language to a select number of languages,
including Haskell, Clean, Rust, Mercury, Scala, Coq, Agda, and Idris.

\paragraph{Typed holes}

Also particularly useful for our purposes is a programming language feature called \emph{typed holes}~\citep{hashimoto1997typed},
as used in Agda~\citep{holesagda},
Idris~\citep{holesidris},
and Haskell~\citep{holeshaskell}.%
~\footnote{As we will explain later, each of these languages are implementations of the \emph{lambda calculus}.}
Typed holes allow one to write an incomplete functional program template,
enabling type inference for any such holes, in turn enabling suggested completions to the user.

Such interactive program synthesis has been labeled as the solver-aided
programming method \emph{sketching}~\citep{gulwani2017program} or
\emph{type-driven development}~\citep{typedrivendev}.

As such, one of our goals is to improve suggested hole completions in such languages to facilitate this interactive programming style.
On the other hand, we may also hook into such existing completion logic to inform our synthesizer.
% TODO: expand on this if in scope, add to follow-up suggestions if out of scope

\subsubsection{Functional programming}

Type-based approaches to synthesis are based on \emph{deductive search}, a top-down search strategy.
Such type-based deductive search is most powerful in a setting where the underlying DSL is loosely-constrained,
that is, permits arbitrary type-safe combinations of subexpressions.

The \emph{functional} paradigm fits 
???

It also offers various general-purpose programming languages,
which helps potentially make our synthesizer potentially applicable to a wide variety of domains.
While there are general-purpose programming languages of other paradigms as well,
the functional programming paradigm has been characterized by its composability or \emph{modularity}~\citep{hughes1989functional},
which is key in the creation of synthesizers that generalize well,
as it encourages \emph{reusing} existing abstractions to allow for a large expressivity using only a small vocabulary,
matching our synthesizer search space requirement of maintaining expressiveness while limiting our search space.

In addition, functional programming is well amenable to programming \emph{types}, allowing advanced constructs such as algebraic datatypes (ADTs)~\citep{burstall1977design}, generalized algebraic datatypes (GADTs)~\citep{dybjer1991logical}, and type classes~\citep{blott1991type}.
While \citet{haskellhistory} provides an overview of these,
the point for our purpose is that these features contribute to expressiveness,
or conversely, help reduce the search space in program synthesis.

As such, program synthesis approaches using types have in fact commonly focused on using \emph{functional} programming languages as the synthesis language.%
~\citep{synquid,eguchi2018automated,scythe,scout,gissurarson2018suggesting,idris,lenses}

\subsubsection{Synthesis language}

% decouple synthesis/target languages

% Type-based approaches to synthesis are based on \emph{deductive search}, a top-down search strategy.
% Such type-based deductive search is most powerful in a setting where the underlying DSL is loosely-constrained,
% that is, permits arbitrary type-safe combinations of subexpressions.
In particular, any ML-like calculus with algebraic datatypes can serve as a core language for type-driven synthesis.~\citep{gulwani2017program}


% I'd lean toward a language with ML framework as the source language (Python/Haskell/OCaml?), and a target language with holes (Haskell/Idris/Agda) and proper type-checks (any but Python/Prolog). none of these used any of those; this intersection means a shared source/target language implies Haskell is the only option and I need to throw out existing implementations anyway. in terms of respectable benchmark, L^2 seems to actually have been used by others before.

% TODO: adjust, already used above
% Note that other work in this area have used differentiable programming languages
% as the synthesis language such as to enable optimization based on
% stochastic gradient descent (SGD)~\citep{forth,terpret}.
% However, such purely SGD-based methods proved less effective than traditional or mixed methods~\citep{terpret}.

% More unfortunately for our purposes, such languages have generally been relatively imperative in nature,
% rendering them less amenable to type-driven synthesis.
Outside of neural methods, programming languages designed for program synthesis also include solver-aided languages~\citep{rosette},
aimed at generating satisfiability conditions for satisfactory programs based on failing input-output examples such as to synthesize program repairs.

% TODO

\subsubsection{Haskell language}

In other words, languages with stronger type systems are preferable for program synthesis.
This is well-known in the traditional program synthesis community,
where various researchers~\citep{synquid,hornclauses,scythe,gissurarson2018suggesting}
have been making use of the Haskell language,
a statically typed, purely functional programming language with type inference and lazy evaluation.~\footnote{\url{https://www.haskell.org/}}
% TODO: mention ML-like (= typed lisp) alternatives e.g. OCaml.

The advantages of Haskell as a synthesis language over the above-mentioned ML lies in its focus on the functional paradigm,
unlike e.g. OCaml: this paradigm is more amenable to static types, which for synthesis helps us constrain our search space.

Another advantage of Haskell over OCaml is it offers typed holes, potentially useful for our purposes to evaluate the type of a given hole.
% TODO: was this needed?
While Idris language offers these benefits as well, Haskell's lazy evaluation seems a more sensible default for our purposes than Idris's strict evaluation, as lazy evaluation brings benefits like being able to deal with infinite data structures.
%~\footnote{Source: \url{https://wiki.haskell.org/Lazy_evaluation}}
~\footnote{As an interesting coincidence, using Haskell for program synthesis also means using an implementation of \citet{lambdacalculus}'s lambda calculus to address \citet{church1957applications}'s problem of synthesizing programs.}

\subsubsection{Grammatical subset}

% TODO: why limited grammar, and limited how

% TODO: explain why I don't feel I need imperative statements or variable definitions
The reason we consider programs of a tree-based form, rather than as a list of imperative statements such as variable definitions,
is that the view of programs as function compositions guarantees us that any complete program will yield us output of the desired type.
This means we view our programs as \emph{pure functions}~\citep{fortran95},
i.e. returning a deterministic output for any given inputs, without performing any additional side effects.
This guarantees that, rather than just branching out, our search will focus on finding acceptable solutions.
This is to be contrasted with \emph{imperative} programs, a coding style characterized by variable mutation,
historically popularized for the purpose of performance optimization from the earlier languages such as assembly.
Synthesis for such languages exists as well~\citep{shi2019frangel}, but is generally harder as it cannot make as much use of types.
% TODO: explain partial programs, how types enable evaluation of these, and why this means we should perform program synthesis based on types

Whereas modern programming languages might have a broad plethora of grammatical constructs available,
for the purpose of our proof-of-concept we will opt to hide much of this.
To achieve this, we will take inspiration from the lambda calculus~\citep{lambdacalculus},
which opts to view constructs as functions,
both so as to provide a unified view of various operations, as well as to enable \emph{currying} of functions.
A \emph{curried}~\citep{currying} version of a function is not unlike its original form,
yet allowing arguments to the function to be applied to it one at a time.
The way this works is that, when an argument is applied to a curried form of a function taking two parameters,
the result is a function that still takes one parameter, before yielding the actual result of the original function.
As such, viewing traditional operators such as addition as curried functions can both simplify the way we view things,
while also increasing expressiveness.
And in fact, this is indeed what happens in various functional languages,
where traditional operators such as \verb|+| are viewed simply as infix operator forms of curried functions.
In Haskell language, for example, \verb|+| may be used to refer to the addition operator in infix notation,
whereas \verb|(+)| may be used to refer to its form as a traditional function.
Other languages such as OCaml, Idris and Agda follow a similar pattern.

As such, we might express as curried functions not only basic mathematical operators,
but by wrapping them as library functions,
also traditional grammatical constructs as (functional) conditionals and function composition operators.
Whereas for legibility purposes, a programmer may prefer to read code using a grammar rich with syntactical sugar,
for the purpose of program synthesis,
it is preferable to decouple the syntax used for synthesis from that presented to the reader,
so as to prevent duplication during synthesis to restrict the search as much as possible.
Instead, a synthesized program might then be converted so as to use a more legible grammar.
For the purpose of this thesis, we will not focus on reader-friendly syntax, however.

Grammar-wise, this should leave us with few options:
% referencing variables (whether from our library or parameters),
referencing variables from our library,
and applying arguments to functions.
For the sake of simplicity,
we will attempt to leave out certain common language features such as defining variables and creating lambda functions.
% ^ TODO: update statement on lambda functions
Hopefully, our idea can be demonstrated without this additional complexity.

To simplify the problem, we would furthermore disallow the use of arbitrary values such as free-style strings.
% Note: a proper solution here probably involves using e.g. https://hackage.haskell.org/package/ad to fix constants by backprop.

As a result, our Haskell-based grammar ends up looking as follows:
\begin{tabular}{ccccc}
    type & constructor & Type Parameters & example & note \\
    Exp & App & Exp Exp & exp exp & \\
    Exp & ExpTypeSig & Exp Type & exp \textbf{::} type & \\
    Exp & Paren & Exp & \textbf{(}exp\textbf{)} & \\
    Exp & Var & QName & qname & \\
    Type & TyApp & Type Type & Type Type & \\
    Type & TyCon & QName & QName & \\
    Type & TyFun & Type Type & Type \textbf{->} Type & \\
    Type & TyVar & Name & name & \\
    QName & UnQual & Name & name & redundant \\
    QName & Special & SpecialCon & specialCon & redundant \\
    Name & Ident & & ident & \\
    SpecialCon & ExprHole & & \textbf{\_} & \\
    SpecialCon & ListCon & & \textbf{[]} & \\
    Exp & Lambda & [Pat] Exp & \textbf{\textbackslash} patt1 patt2 ... \textbf{->} exp & lambdas \\
    Pat & PVar & Name & name & lambdas, redundant \\
    Exp & Let & Binds Exp & \textbf{let} binds \textbf{in} exp & lambdas typed without ScopedTypeVariables \\
    Binds & BDecls & [Decl] & decl1\textbf{;} decl2\textbf{;} ... & lambdas typed without ScopedTypeVariables \\
    Decl & PatBind & Pat Rhs (Maybe Binds) & pat \textbf{=} rhs & lambdas typed without ScopedTypeVariables \\
    Rhs & UnGuardedRhs & Exp & exp & lambdas typed without ScopedTypeVariables, redundant \\
\end{tabular}
% TODO: use BNF?
% TODO: make this not specific to haskell by simplifying out the haskell-specific bits redundant here in my simple version

Our programs are structured as follows:

\begin{verbatim}
    let
        foo = expr
        bar = expr
        ...
    in body
\end{verbatim}

\subsubsection{Operator whitelist}

Our synthesis approach itself is agnostic to the set of operators used,
allowing for relatively straight-forward experimentation with different sets of operators.
In fact, adding new operators largely concerns dataset generation, and is only a matter of:
\begin{itemize}
    \item adding the actual operators, together with their definitions;
    \item updating the list of used types for randomized type instantiation;
    \item updating random generation to ensure it incorporates used types;
    \item running the dataset generation to ensure our synthesizer has data from which to learn about our new operators;
    \item actually training our synthesizer using our updated dataset including our new operators.
\end{itemize}

% TODO: which operators and why

\subsection{Search technique}

As we are interested in types, we will need to build upon AST-based (or node-based) rather than sequential (or token-based) synthesis methods.

% TODO: explain why supervised instead of RL / hybrid

As a benchmark algorithm we will therefore use the neuro-symbolic program synthesis method introduced in \citet{nsps} and further explained in section \ref{sec:nsps},
a top-down incremental neural synthesis method which like other neural methods is not presently making use of type-level information.

The reason we picked this as our benchmark in particular is that there have been only few neural synthesizers out there based on abstract syntax trees (ASTs), rather than sequences.
This is important for us because we can apply types to an AST containing holes, though we may not be able to for any sequence representing a partial program.

This method is based on two novel neural modules.
The first module, called the \emph{cross correlation I/O network}, given a set of input-output examples, produces a continuous representation of the set of I/O examples.
The second module, the \emph{Recursive-Reverse-Recursive Neural Network} (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs.~\citep{nsps}

\subsubsection{Types}

% why types? differences from NLP, CS way, not reflected in ASTs in e.g. variables
% how to supervise with these? representation?

To get the most out of our types, we will want to provide them for:
\begin{itemize}
    \item inputs
    \item outputs
    \item abstract syntax trees (ASTs)
\end{itemize}

The basic idea here is simple: our program should return the desired type, while taking the desired input types.
% BNFs

However, in the simple case, the implications of this info do not require much computation:
one will just restrict their search to those AST nodes allowed for the present hole as dictated by the grammar.

\subsubsection{Compiler / type feedback}

\citep{bunel2018leveraging} condition on syntax to generate only correct programs, see their section 5.

\pagebreak

\section{Experiment} % \label{sec:experiment}

\subsection{Benchmark task}

% benchmark task:
% - [x] typed MIL: Prolog/Python, types: poly list, mono int/char. allows recursion.
% - [x] Myth: Haskell/OCaml, types: poly list/tree, mono bool/nat. third-party repo available but errors. partial official code. allows recursion. components not listed in the paper~\citep{tamandu}.
% - [x] lambda^2: OCaml/Python, types: poly list/tree, mono bool/num. permissive grammar involving pattern matching, recursion, and a flexible set of primitive operators and constants.
% - [x] NPI/AlphaNPI: task is sequential :(
% - [x] NSPS: flashfill so few parametric types :(, also tasks not publicized
% - DILP?: paper tests induction rather than synthesis, so adapting their benchmark makes it hard to compare -- just rules, no trees or types :(
% - [x] DeepCoder: dataset (/code) not published :(
% - [x] Suggesting Valid Hole Fits: based on types, not examples :(
% - [x] Synquid / Polymorphic Succinct Types: based on refinement types instead of i/o examples :(
% - [x] Scythe: no code :(, uses types + i/o examples. no (own) task! :(
% - [x] Scout: no code, and only extended abstract available, with no info on task :(
% - [x] Idris: based on types, not examples :(
% - [x] Houdini: tasks mostly involve neural networks so suck to evaluate :(
% - [x] Terpret: 12 tasks (table 2), complex grammar (figure 3) incl constants on probabilistic language, so seems kinda different, plus quite binary so not very type-ey :(
% - [x] Tamandu: PBE with type-based pruning, but non-neural, so only evaluated on 23 functions. allows recursion. types so simple I can't add value. :(
% https://docs.google.com/spreadsheets/d/1uDA9suwASDzllxJZDt--wZ0ci7q4eJIfPcAw9qr18-U/edit?usp=sharing

We had trouble finding suitable benchmark task in the literature.

On the one hand, especially benchmark tasks within non-neural program synthesis methods have usually consisted of only a limited number of tasks%
~\citep{myth,lambda2,typedmil,houdini,tamandu,dilp}~\cite{terpret},
as such non-neural methods do not require an equivalent to the training set typically used in machine learning setups.

% While \cite{lambda2} did not use neural methods,
On the other hand, benchmark tasks within Neural Program Synthesis have often remained somewhat simpler in nature,
rendering them less fit for our purposes --- either being imperative in nature (and as such being less amenable to types),
such as sorting tasks~\citep{npi,alphanpi},
or otherwise focusing on a simple set of types --- such as strings~\citep{nsps} ---%
or differently put, not including building blocks with parametric return types.

As a third issue, it was also common for existing papers to keep their datasets unpublished~\citep{nsps,deepcoder}.

As a result, it looks like we would need to create a benchmark task, or if anything a training dataset compatible with one of the existing benchmark tasks, ourselves.

Now, for compatibility we may simply ensure our test sets contain the benchmark tasks of existing papers.
% To achieve this, we may increasingly add types, such as to support, in order of complexity:
% \begin{itemize}
%     \item list, bool~\citep{terpret}
%     \item int~\citep{tamandu}
%     \item char~\citep{typedmil}
%     \item nat, tree~\citep{myth,lambda2}
% \end{itemize}

To constrain the engineering scope of our experiment, it would seem reasonable to use a benchmark paper similarly limited in grammar, so as to reduce complexity not required for the experiment where possible.

% By this reasoning, we would like to benchmark against \citet{tamandu}, which "[focuses] on the synthesis of purely functional programs without any lambda expressions, explicit recursion or conditionals (i.e., only function application is allowed)".

% Other papers considered as potential benchmarks often featured more comprehensive grammars, typically incorporating function recursion~\citep{typedmil,myth}, if not also pattern matching and a flexible set of primitive operators and constants~\citep{lambda2}.

We will do this by taking libraries of basic operations in accordance with these papers, then, in order to create a training set, generating any possible functions using combinations thereof within a given complexity threshold.

The hole filling is as follows:
\begin{itemize}
    % TODO: handle parameters ensuring their use
    \item we take our function, containing a number of parameters, a return type, and a body that contains holes;
    \item we enumerate the variables we have that could (if applicable, after function application) return a type compatible with our desired return type, to create versions of the function with said hole plugged with these variables (which in the case of functions, may, in turn, contain holes);
    % TODO: queue or whatever, any data structure really
    % TODO: distinguish unfilled currying slot vs unfilled holes
    \item we filter these potential programs given their number of remaining holes based on our complexity threshold;
    % TODO: specify complexity threshold
    \item we add the remaining programs to a queue of complete or partial programs, depending on whether they still contain holes;
    \item we similarly process the functions in the queue of partial programs, until we are left with an enumeration of complete programs;
    \item we evaluate the complete programs by the input-output examples, to filter down to those matching our behavioral criteria;
    \item finally, we generate sample inputs for each program, and calculate their corresponding outputs.
    % TODO: consider a depth-first alternative for lower memory complexity
    % TODO: compare to approach used in https://github.com/minori5214/programming-by-example/blob/master/generate.py
\end{itemize}

The process for generating task functions to train our synthesizer on is as follows:
\begin{itemize}
    \item we start using a body consisting of a hole, typed with a wildcard;
    \item we use hole-filling to generate potential complete programs within a certain complexity threshold.
    % TODO: compare to approach used in https://github.com/minori5214/programming-by-example/blob/master/generate.py
\end{itemize}

We additionally considered alternative methods of generating task functions:
\begin{itemize}
    \item generating a function based on a type signature;
    \item generating a function based on an input parameter type; % , then filling holes, only afterwards seeing what return type ends up coming out.    
\end{itemize}
There are two reasons we decided against using these type-first approaches:
\begin{itemize}
    \item as they impose constraints on the functions generated, they will end up generating less functions given the same amount of compute;
    \item if we wanted to allow generating more general versions of the desired function, then pre-fixing the types such as not to allow more generic implementations feels counter-productive, if not anti-thetical to the spirit of functional programming.
\end{itemize}

The synthesis process itself is as follows:
\begin{itemize}
    \item we take the type of our function, along with a body consisting of a hole;
    \item we use hole-filling to generate potential complete programs;
    \item we generate sample inputs for each program, and calculate their corresponding outputs;
    \item we evaluate the complete programs by the input-output examples, to filter down to those matching our behavioral criteria;
    % TODO: consider a depth-first alternative for lower memory complexity
\end{itemize}

% TODO: EXPERIMENT
% TODO: elaborate how to supervise existing methods with type info

\pagebreak

\section{Result} % \label{sec:result}

Having added our type-level supervision during training, we expect synthesis success rates to rise and required evaluations to drop compared to the baseline algorithm.
This demonstrates that the findings from traditional program synthesis methods are relevant also in the field of neural program synthesis.

% TODO: RESULT

\pagebreak

\section{Discussion} % \label{sec:discussion}

One question we aim to answer here is whether our approach can meaningfully scale to program sizes not explicitly train on.
% TODO: check what claims NSPS made on this.

\subsection{Topics for future research}
\begin{itemize}
    \item How can we apply a more generalized program synthesis model to a specific problem? Can we take inspiration here from neural network pruning?
    % \item ...
\end{itemize}

\pagebreak

\nocite{*}
% \bibliographystyle{acm}
\bibliographystyle{plainnat}  % fixes https://tex.stackexchange.com/questions/166840/why-do-i-get-author-when-i-use-citet-with-natbib
\bibliography{references}

\end{document}
