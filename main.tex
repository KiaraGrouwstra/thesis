\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx, color}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{hyperref}
\usepackage[numbers]{natbib}
\usepackage{subfiles}

% \title{thesis proposal UvA MSc AI}
% \author{Tycho Grouwstra}
% \date{August 2019}

\begin{document}

\subfile{title-page-ai.tex}

\section{Research Direction}

% synthesis
Program synthesis is the task to automatically construct a program that satisfies a given high-level specification,
be it a formal specification, a natural language description, or input-output examples.
% ~\footnote{\url{https://en.wikipedia.org/wiki/Program_synthesis}}

This differs from program induction, commonly known as 'supervised learning',
% TODO: SOURCE DEFINITIONS
in that fitting the model into the discrete form of a given grammar
forces one to instantiate such a model from probabilistic data interpretations,
whereas in traditional supervised learning,
such probabilistic interpretations of the data may be taken at prediction time.
% 
This means that in program synthesis, we are able to distill our modeled function
to a simplified discrete form that may well be intelligible to humans as well,
opening up opportunities for human-machine cooperation in writing software.
% 
And in fact, in a novel functional programming paradigm called
\emph{type-driven development}~\footnote{\url{https://manning.com/books/type-driven-development-with-idris}}
based on typed holes, this is already happening.

% challenges
While considered a holy grail of computer science, program synthesis is a challenging task, characterized by large search spaces, e.g. a space of $10^{5943}$ programs discovering an expert implementation of the MD5 hash function.~\cite{gulwani2017program}

% approaches
Program synthesis was traditionally studied as a computer science problem, but has recently been explored using machine learning approaches under the name of neural program synthesis~\citep{nps}.
However, there has not yet been much work combining these two approaches.

% CS vs ML
Whereas the traditional approaches in program synthesis focused on constraining the large discrete search space, neural program synthesis instead tends to incrementally generate programs, using continuous representations of the state space to predict the next token, be it in a sequential fashion~\citep{alphanpi}, or in a structured one based on abstract syntax trees (ASTs)~\citep{nsps}.

While the typically incremental approach of neural program synthesis breaks down the problem by considering one part of the program at a time, the original challenge remains.

Whereas a program synthesizer may be programmed or taught to output programs adhering to a given grammar, there is typically no guarantee that such partial program constructions will qualify as a full executable program adherent to the grammar.
As a result, neural synthesizers will have little intermediary feedback to go by, limiting their effectiveness.

But if only complete programs can be evaluated for validity and behavior, then 
% for the duration of much of the incremental synthesis, the synthesizer has little feedback to go by.
it is much harder to provide synthesizers with an accurate understanding of incomplete programs.
% TODO: CONJECTURE, CITATION NEEDED?

We hypothesize that the effectiveness of neural program synthesis may be improved by 
% supervising incremental predictions offering additional features 
adding type information as additional features during training.
% why types? differences from NLP, CS way, not reflected in ASTs in e.g. variables
% how to supervise with these? representation?

% supervising using top-level type by inference through parametric return types (parametric polymorphism)? check parametrically typeable task ops e.g. from DILP?
% sequential monadic shit from DeepMind papers is useless...

% node-level types: probably matches how AI techniques are filtering already, but hard to confirm given lacking source code for NSPS

\emph{Research question: can neural program synthesis methods benefit from using types as additional features?}
% We hypothesize that the effectiveness of neural program synthesis may be improved by adding type information as additional features during training.
% TODO: add second question + research?
% TODO: try other program induction methods (used to reduce the search space) to apply for neural methods.

\section{Expected Contribution}

The present work aims to be the first experiment to:
\begin{itemize}
    \item bring the type-based information traditionally used in program synthesis into the newer branch of neural program synthesis;
    \item bridge the two fields, such as to find a best-of-both-worlds golden mean;
    \item use this type info to better constrain the search space, improving the effectiveness of (neural) program synthesis methods. 
\end{itemize}

\section{Existing work}

% % https://alexpolozov.com/blog/program-synthesis-2018/

% % TODO: LITERATURE REVIEW

\section{Experiment}

% PBE
User intent in program synthesis can be expressed in various forms, including logical specification, examples, traces, natural language, partial programs, or even related programs.~\citep{gulwani2017program}
Program specifications in program synthesis may include formal program specifications, natural language descriptions of the program, as well as input-output examples, aka programming by example (PBE).
For our purposes here, we will focus on PBE, as having known input-output types will help provide us the needed type information we would like to use.

% structure
As we are interested in types, we will need to build upon AST-based (or node-based) rather than sequential (or token-based) synthesis methods.
% why?
1
Type-based approaches to synthesis are based on \emph{deductive search}, a top-down search strategy.
Such type-based deductive search is most powerful in a setting where the underlying DSL is loosely-constrained, that is, permits arbitrary type-safe combinations of subexpressions. In particular, any ML-like calculus with algebraic datatypes can serve as a core language for type-driven synthesis.~\citep{gulwani2017program}

% holes
Particularly useful for our purposes is a programming language feature called \emph{typed holes}~\citep{hashimoto1997typed},
as used in Agda~\footnote{\url{https://agda.readthedocs.io/en/latest/getting-started/quick-guide.html}},
Idris~\footnote{\url{http://docs.idris-lang.org/en/latest/tutorial/typesfuns.html\#holes}},
and Haskell~\footnote{\url{https://wiki.haskell.org/GHC/Typed_holes}}.
Typed holes allow one to write an incomplete functional program template,
enabling type inference for any such holes, in turn enabling suggested completions to the user.
Such interactive program synthesis has been labeled as the solver-aided
programming method \emph{sketching}~\citep{gulwani2017program} or
\emph{type-driven development}.
% ~\footnote{\url{https://manning.com/books/type-driven-development-with-idris}}
As such, one of our goals is to improve suggested hole completions in such languages to facilitate this interactive programming style.

% types
To get the most out of our types, we will want to provide them for:
\begin{itemize}
    \item inputs
    \item outputs
    \item abstract syntax trees (ASTs)
\end{itemize}

The basic idea here is simple: our program should return the desired type, while taking the desired input types.
% BNFs

However, in the simple case, the implications of this info do not require much computation: one will just restrict their search to those AST nodes allowed for the present hole as dictated by the grammar.

% generics
What makes the use of types more powerful in restricting the search space is the use of \emph{parametric polymorphism}: a function $append$ may work using either lists of numbers or lists of strings.
~\footnote{Left out of scope here are \emph{refinement types}, which further aid synthesis based on conditions.}
As such, its type signature may be made \emph{generic} such as to have its return type reflect the types of the parameters used. Having such information available at the type level may add additional information over what is used in the simpler case above.

It should be noted however that not all programs benefit from this, but only those for which potential building blocks include statically typed functions with parametric return types.

% decouple synthesis/target languages
As a result, this restriction limits our method from being used for direct synthesis of programming languages that are dynamically typed, e.g. Ruby language, as well as to those lacking parametric polymorphism, e.g. C language.
However, this only partly hinders those wishing to synthesize programs in such languages: rather than synthesizing in such languages directly, we would suggest synthesizing from the language that best constrains the search space, then using source-to-source translation methods (e.g. neural machine translation~\citep{kalchbrenner2013recurrent})
using the synthesized program as input to obtain a program translated into the target language.
% TODO: make into experiment hypothesis?

% haskell
In other words, languages with stronger type systems are preferable for program synthesis.
This is well-known in the traditional program synthesis community, where various researchers
% TODO: CITE
have been making use of the Haskell language, a statically typed, purely functional programming language with type inference and lazy evaluation.~\footnote{\url{https://www.haskell.org/}}
% https://en.wikipedia.org/wiki/Haskell_\%28programming_language\%29
% TODO: mention ML-like (= typed lisp) alternatives e.g. OCaml.

The advantages of Haskell as a synthesis language over the above-mentioned ML lies in its focus on the functional paradigm, unlike e.g. OCaml: this paradigm is more amenable to static types, which for synthesis helps us constrain our search space.

% TODO: benchmark algorithm
% - NSPS is closed due to IP issues -_-
% - NPI variants are sequence-based rather than AST-based
% - Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design (http://proceedings.mlr.press/v80/lyu18a/lyu18a.pdf)
% - Programmatically Interpretable Reinforcement Learning (https://arxiv.org/pdf/1804.02477.pdf)
% - Towards Mixed Optimization for Reinforcement Learning with Program Synthesis (https://arxiv.org/pdf/1807.00403.pdf)
% - Program Synthesis using Conflict-Driven Learning (https://arxiv.org/pdf/1711.08029v1.pdf)

As a benchmark algorithm we will use \cite{nsps}, a top-down incremental neural synthesis method which like other neural methods is not presently making use of type-level information.
% TODO: swap, no code available

% benchmark task:
% 🙆:
% - typed MIL: Prolog/Python. types: poly list, mono int/char.
% - Myth: Haskell/OCaml. types: poly list/tree, mono bool/nat.
% - λ^2: OCaml/Python. types: poly list/tree, mono bool/num.
% I'd lean toward a language with ML framework as the source language (Python/Haskell/OCaml?), and a target language with holes (Haskell/Idris/Agda) and proper type-checks (any but Python/Prolog). none of these used any of those. this intersection means a shared source/target language implies Haskell is the only option and I need to throw out existing implementations anyway. in terms of respectable benchmark, λ^2 seems to actually have been used by others before.
% 🙅:
% - NPI/AlphaNPI: task is sequential :(
% - NSPS: flashfill so few parametric types :(
% - DILP?: paper tests induction rather than synthesis, so adapting their benchmark makes it hard to compare. just rules, no trees or types. :(
% - DeepCoder: dataset (/code) not published? :(
% - Tamandu: no code? :(
% - Suggesting Valid Hole Fits: based on types, not examples. :(
% - Synquid / Polymorphic Succinct Types: based on refinement types instead of i/o examples. :(
% - Scythe: no code?, based on refinement types instead of i/o examples. :(
% - Scout: no code? :(
% - Idris: based on types, not examples. :(
% - Houdini: tasks mostly involve neural networks so suck to evaluate. :(

As a benchmark task we will use \cite{lambda2}'s corpus of over 40 data structure manipulation tasks involving lists, trees, and nested data structures such as lists of lists or trees of lists.
As their synthesis language \cite{lambda2} used the OCaml language~\footnote{\url{https://ocaml.org/}}, an industrial-strength programming language supporting functional, imperative and object-oriented styles.

While \cite{lambda2} did not use neural methods, benchmark tasks within Neural Program Synthesis have often remained somewhat simpler in nature, rendering them less fit for our purposes --- either being imperative in nature, such as sorting tasks~\citep{npi}, or otherwise not including building blocks with parametric return types~\citep{nsps}.

% TODO: fix benchmark task based on algo?

% TODO: EXPERIMENT
% TODO: elaborate how to supervise existing methods with type info

\section{Result}

Having added our type-level supervision during training, we expect synthesis success rates to rise and required evaluations to drop compared to the baseline algorithm. This demonstrates that the findings from traditional program synthesis methods are relevant also in the field of neural program synthesis.

% TODO: RESULT

\bibliographystyle{acm}
\bibliography{references}

\end{document}
