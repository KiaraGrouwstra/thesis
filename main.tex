\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx, color}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{hyperref}
\usepackage[numbers]{natbib}
\usepackage{subfiles}
\usepackage{refcheck}
\usepackage{csquotes}

\begin{document}

\subfile{title-page-ai.tex}

\tableofcontents

\pagebreak

\section{Research Direction} % \label{sec:research-direction}

\subsection{Program Synthesis}

\begin{displayquote}
    truly solving program synthesis is the last programming problem mankind will have to solve~\citep{nps}
\end{displayquote}

\begin{displayquote}
    If artificial intelligence is software 2.0~\citep{software20},
    then \emph{program synthesis} is software 2.0 applied to the field of software development itself.
\end{displayquote}
% I think only few will feel offended by the implication all program synthesis uses AI.

% synthesis
\emph{Program synthesis}~\citep{church1957applications} is the task to automatically construct a program%
~\footnote{While the definition of a program might be debatable as well, and there has been work on predicting type signatures from function names~\citep{wang2018predicting}, for the purpose of our paper we will focus on traditional executable programs, rather than e.g. 'programs' describing types.}%
that satisfies a given high-level specification,
be it a formal specification, a natural language description, full program \emph{traces}, or input-output examples.
% ~\footnote{\url{https://en.wikipedia.org/wiki/Program_synthesis}}

\subsection{Related fields}

\subsubsection{Program Induction}

% TODO: find less ambiguous terminology
This differs from program induction, like supervised learning more generally,
in that fitting the model into the discrete form of a given grammar
forces one to instantiate such a model from probabilistic data interpretations,
whereas in traditional supervised learning,
such probabilistic interpretations of the data may be taken at prediction time.
As such, program synthesis explicitly returns a program, whereas program induction learns to \emph{mimic} it rather than explicitly return it.~\citep{nps}

\subsubsection{Constraint satisfaction vs. discrete optimization}

% relation to https://en.wikipedia.org/wiki/Constraint_programming : that seems a subset associated with logic programming
% relation to SMT: that's first-order logic, synthesis is second-order
% relation to SAT: that's SMT without linear constraints, so even less expressive
While its definition frames program synthesis as a type of constraint satisfaction problem (CSP),
% ~\footnote{https://en.wikipedia.org/wiki/Constraint_satisfaction_problem}
some methods may instead opt to approach it as a discrete optimization problem,
% relation to https://en.wikipedia.org/wiki/Combinatorial_optimization : that one assumes a finite set of objects? 
as specifications such as input-output examples allow us to count the examples our candidate program satisfies.

Intuitively, a program satisfying part of our examples may be regarded as closer to a solution than one that does not satisfy as many.
Furthermore, additional considerations such as performance may further push as to find a solution that not only calculates outputs correctly but also runs within reasonable time or memory constraints.

\subsection{Goal: human-machine cooperation}

This means that in program synthesis, we are able to distill our modeled function
to a simplified discrete form that may well be intelligible to humans as well,
opening up opportunities for human-machine cooperation in writing software.
Specifically, this will allow machines to improve on programs written by humans, and the other way around.
As such, program synthesis may bring \emph{hybrid intelligence} to the field of software development.

And in fact, in a novel development paradigm in functional programming called \emph{type-driven development}%
~\citep{brady2017type}, based on typed holes, this is already happening.

\subsection{Approaches}

\subsubsection{Synthesis from formal specifications}

Program synthesis was traditionally studied as a computer science problem,
where the problem was typically framed using a formal specification,
which was then tackled using e.g. an enumerative search, deductive methods, or constraint-solving techniques.~\citep{gulwani2017program}
However, such formal specifications ended up about as hard to write as the original program,
rendering this approach to the problem not very useful.

\subsubsection{Programming by Example}

Instead, it was found that for users, input-output examples were a more attractive way to specify desired program behavior,
although as an incomplete specification this made for a harder problem from the perspective of the synthesizer.~\citep{bodik2013algorithmic}
This field is named \emph{Programming by Example} (PBE).
As the specification is incomplete here, PBE is considered \emph{inductive} synthesis, as opposed to the \emph{deductive} synthesis when we do have a complete specification.
In other words, from the perspective of the synthesizer, PBE is generally a more difficult problem.
PBE may be further split up according to the target programming language to be synthesized~\citep{bodik2013algorithmic},
notably logic programs (e.g. Prolog language) versus functional programs (e.g. Lisp, Haskell).
Our interest here is primarily in synthesis of functional programs.
PBE was traditionally characterized by methods such as~\citep{gulwani2017program}:
\begin{description}
    \item[Version Space Algebras (VSAs)]
        constraining grammar productions with examples
    \item[deduction-based techniques]
        inspired by those from synthesis based on formal specifications
    \item[ambiguity resolution]
        requesting user input to resolve potential ambiguities
\end{description}

\subsubsection{Neural program synthesis}

More recently however, PBE has been explored using machine learning approaches, under the name of neural program synthesis~\citep{nps},
typically using e.g. sequence-to-sequence techniques supplemented with mechanisms such as attention and memory~\citep{nps},
though there have also been approaches framing program synthesis by representing programs as \emph{abstract syntax trees} (ASTs) rather than as sequences~\citep{nps,odena2020learning}.

Unfortunately though, program synthesis has been less straight-forward to tackle this way than other AI problems,
as our search space is typically discrete, meaning we cannot simply apply gradient-based optimization.~\citep{nps}

\subsubsection{Hybrid approaches}

However, while the concensus seems to be that neither traditional nor pure machine learning based approaches are likely to solve program synthesis in isolation,
making a \textbf{hybrid system} the most promising path forward~\citep{nps},
the specifics on how to best combine these approaches has remained the subject of active research.

\subsubsection{Differences} % CS vs ML

Whereas traditional approaches in program synthesis (and particularly PBE) focused on constraining the large discrete search space,
such as deductive and constraint-solving approaches,
neural program synthesis instead tends to incrementally generate programs,
using continuous representations of the state space to predict the next token,
be it in a sequential fashion~\citep{alphanpi},
or in a structured one based on abstract syntax trees (ASTs)~\citep{nsps}.

\subsection{Challenges}

While considered a holy grail of computer science~\citep{gulwani2017program}, program synthesis is a challenging task, characterized by large search spaces,
e.g. a space of $10^{5943}$ programs discovering an expert implementation of the MD5 hash function.~\cite{gulwani2017program}

\subsubsection{Challenges of type-based synthesis}

While types can be used as a powerful way of specifying program behavior through the use of e.g. refinement types~\citep{synquid} or succinct types~\citep{guospeeding},
these typically end up in a similar pitfall as program synthesis based on formal specifications,
requiring the user to essentially write a type that may end up similar in complexity to the actual program itself.

Types of a level that users may be more familiar with, which may include polymorphic types or even algebraic data types,
may end up not sufficiently expressive: while certainly constraining the program space,
examples may still be needed to disambiguate between potential candidate programs.

\subsubsection{Challenges of programming by example}

Whereas a program synthesizer may be programmed or taught to output programs adhering to a given grammar,
there is typically no guarantee that such partial program constructions will qualify as a full executable program adherent to the grammar.
As a result, neural synthesizers will have little intermediary feedback to go by, limiting their effectiveness.

But if only complete programs can be evaluated for validity and behavior, then 
we will be ill-equipped to provide synthesizers with an accurate understanding of partial programs,
which make up for a large part of our prediction steps.

\subsubsection{Challenges of neural program synthesis}

While the typically incremental approach of neural program synthesis breaks down the problem of PBE by considering one part of the program at a time, the original challenge remains:
our search space typically remains large, meaning it will not be viable to proportionally scale our training sets by program size.
As such, it would be great if we could somehow supervise the intermediate prediction steps.

\subsection{Research question}

\subsubsection{Complementary strengths}

Our key observation here is thus that input-output examples and types are quite complementary as specifications constraining our program behavior:
whereas input-output examples are relatively expressive, they may only help us to evaluate the quality of complete programs;
types, on the other hand, are by themselves not usually descriptive enough of our task,
but fortunately, due to typed holes,
% TODO: rewrite stuff on typed holes since I'm technically not even using those yet atm...?
may help us to evaluate and inform further incremental synthesis steps even of incomplete functions still containing holes.

\subsubsection{Hypotheses}

We therefore hypothesize that program synthesizers may thus capitalize on this synergy by utilizing both types of information,
rather than settling for only one of the two, as most existing methods have done.%
~\footnote{
    While one might wonder if this constrains our idea to the subset of PBE problems where type information is available,
    this limitation is essentially meaningless: when one has input-output examples, one should generally be able to infer their type.
    This would render our idea applicable for practically any (neural) methods for PBE.
}%
~\footnote{
    Whereas one might wonder if a focus on types may constrain our method to synthesizing statically typed languages,
    in the functional paradigm in particular, we believe there are workarounds to this.
    While we certainly believe users may find switching to a statically typed functional language to be beneficial~\citep{hughes1989functional},
    an alternative would be to (1) have one algorithm translate the partial program to a statically typed language,
    (2) complete the program using program synthesis from there,
    then finally, (3) using a third algorithm, translate the full program back to the target language.
}

Specifically, we hypothesize that the effectiveness of neural program synthesis may be improved by
adding type information as additional features during training and testing,
which in AST-based neural program synthesis may help supervise each incremental prediction step.

\begin{displayquote}
    \emph{Research question \#1: can neural program synthesis methods benefit from using types as additional features?}
\end{displayquote}

Furthermore, in an incremental node-based synthesis approach, we hypothesize that after each synthesis step,
in languages featuring typed holes we may pre-compile even partial programs such as to provide the synthesizer with immediate feedback.

\begin{displayquote}
    \emph{Research question \#2: can neural program synthesis methods benefit from using compilation checks as additional features?}
\end{displayquote}

% An alternative approach would be to enumerate any possible options for a given node,
% then filtering this list of options based on such compiler feedback.%
% ~\footnote{One potential concern here is the scalability of such enumeration as the number of valid options grows.}

% This is indeed done for type-based program synthesis~\citep{myth},
% and we hypothesize neural synthesis approaches may benefit from such pre-filtering as well,
% reducing synthesis to a ranking problem of (partial) candidate programs.

% \begin{displayquote}
%     \emph{Research question \#3: can neural program synthesis methods benefit from type-based filters by reframing the problem to learning-to-rank?}
% \end{displayquote}

% Lastly, we hypothesize that these added features still offer added value over deduction-based approaches aiming to push input-output examples through the grammar (investigate separately for datasets without and with many functions that are not invertible)

% \begin{displayquote}
%     \emph{Research question \#4: can deduction-based neural program synthesis methods benefit from using types and compilation checks as additional features?}
% \end{displayquote}

\section{Expected Contribution} % \label{sec:expected-contribution}

The present work aims to be the first experiment to:
\begin{itemize}
    \item bring the type-based information traditionally used in program synthesis into the newer branch of neural program synthesis;
    \item bridge the two fields, such as to find a best-of-both-worlds golden mean;
    \item use this type info to better constrain the search space, improving the effectiveness of (neural) program synthesis methods;
    \item offer an open-source implementation of the algorithm described in \citet{nsps};
    \item generate a dataset for neural synthesis of functional programs;
    \item lay out a way of generating such datasets, including an open-source implementation, addressing the current reliance on hand-crafted curricula~\citep{nps}.
\end{itemize}

\section{Existing work} % \label{sec:existing-work}

Methods used in program synthesis include:
\begin{itemize}
    \item approaches based on enumerative search~\citep{akiba2013calibrating,alur2013syntax,alur2016sygus};
    \item oracle-guided synthesis~\citep{solar2008program}, based on the idea of splitting the synthesis task into generating and filling \emph{sketches} of programs;
    \item constraint solving methods such as satisfiability modulo theories (SMT) solvers~\citep{rosette,architecture} and conflict-driven learning~\citep{feng2018program,hornclauses};
    \item deductive search techniques, based on theorem provers, recursively reducing the synthesis problem into sub-problems, propagating constraints, including approaches based on inverse semantics of DSL operators~\citep{flashmeta,prose} and type-theoretic PBE~\citep{myth,synquid};
    \item sequence-to-sequence methods~\citep{npi,neuralmachinetranslation,alphanpi}, leveraging techniques from natural language processing (NLP) to represent program synthesis as a sequence prediction problem, often combined with mechanisms such as convolutional recurrence~\citep{neuralgpu}, attention~\citep{nmt,ptrnets,structuredattention}, memory~\citep{ntm,neuralram,neuralprogrammer,hierarchicalmemory}, function hierarchies~\citep{npi,npl}, and recursion~\citep{cai2017making};
    \item tree-based approaches such as recursive neural networks~\cite{nsps};
    \item graph neural networks~\citep{allamanis2017learning,brockschmidt2018generative}, which aim to model relations between variable occurrences;
    \item evolutionary algorithms~\citep{koza1994genetic}, which generate programs similar to existing promising candidates;
    \item reinforcement learning~\citep{chen2017towards,bunel2018leveraging,xu2019neural,camacho2019towards}, learning to synthesize by trying rather than from supervision signals;
    \item learning-to-rank methods~\citep{singh2015predicting}, which use techniques from information retrieval (IR) to rank a satisfactory candidate program on top;
    \item bayesian optimization~\citep{looks2005learning,verma2018programmatically}, which aims to find an optimal solution within the shortest number of attempted programs by framing synthesis as a discrete optimization problem;
    \item active learning~\citep{shen2019using}, intended to efficiently request interactive user feedback.
\end{itemize}

% TODO: tell about some existing works, kinda argue why I wanna do my thing instead of them being good enough already
% TODO: expand on Myth paper

\section{Methodology} % \label{sec:methodology}

As per \citet{gulwani2017program}, a synthesizer is typically characterized by three key dimensions:
\begin{itemize}
    \item the kind of \emph{constraints} that it accepts as expression of \emph{user intent};
    \item the \emph{space} of programs over which it searches;
    \item the \emph{search technique} it employs.
\end{itemize}

For our purposes here, we will focus on synthesis based on input-output examples, aka programming by example (PBE),
as having known input-output types will help provide us the needed type information we would like to use.

\subsection{User intent}

As the constraints to express user intent, we would like to use input-output examples, which may be considered a compromise between what is easier for the \emph{end-user}, which may ideally prefer natural-language descriptions of program behavior, versus what is easier for the \emph{synthesizer}, which may ideally prefer a complete formal specification of program behavior.

\subsection{Program search space}

The program search space consists of the synthesis language (defined by a \emph{context-free grammar}), either general-purpose or a \emph{domain-specific language} (DSL), potentially further restricted to a subset of its original operators, such as by providing a whitelist of \emph{operators}.

The trade-off here is one of expressiveness (can we write a satisfactory program, ideally using not too much code?) versus limiting our search space (ensure we can find a solution within too long).
Within this context, it would seem preferable to pick a limited grammar in the functional programming paradigm using static typing.

% why functional
% why static typing
% why limited grammar, and limited how

% \subsubsubsection{Synthesis language}

% PBE
User intent in program synthesis can be expressed in various forms, including logical specification~\citep{temporalstreamlogic},
examples, traces, natural language~\citep{abstractsyntaxnetworks}, partial programs, or even related programs.~\citep{gulwani2017program}
%Program specifications in program synthesis may include formal program specifications, natural language descriptions of the program, as well as input-output examples, aka programming by example (PBE).
A synthesizer may be passed additional information in various ways:
\begin{itemize}
    \item further examples;
    \item more descriptive types~\citep{synquid};
    \item additional features describing properties of the function~\citep{odena2020learning}.
\end{itemize}

Such logical specifications might also include types of inputs and outputs,
and this branch of program synthesis has commonly focused on using functional programming languages as the synthesis language.%
~\citep{synquid,eguchi2018automated,scythe,scout,gissurarson2018suggesting,idris,lenses}

% Synthesis language
Note that other work in this area have used differentiable programming languages
as the synthesis language such as to enable optimization based on
stochastic gradient descent (SGD)~\citep{forth,terpret}.
However, such purely SGD-based methods proved less effective than traditional or mixed methods~\citep{terpret}.
% More unfortunately for our purposes, such languages have generally been relatively imperative in nature,
% rendering them less amenable to type-driven synthesis.
Outside of neural methods, programming languages designed for program synthesis also include solver-aided languages~\citep{rosette},
aimed at generating satisfiability conditions for satisfactory programs based on failing input-output examples such as to synthesize program repairs.

\subsection{Search technique}

As we are interested in types, we will need to build upon AST-based (or node-based) rather than sequential (or token-based) synthesis methods.

As a benchmark algorithm we will therefore use \cite{nsps}, a top-down incremental neural synthesis method which like other neural methods is not presently making use of type-level information.

The reason we picked this as our benchmark in particular is that there have been only few neural synthesizers out there based on abstract syntax trees (ASTs), rather than sequences.
This is important for us because we can apply types to an AST containing holes, though we may not be able to for any sequence representing a partial program.

This method is based on two novel neural modules.
The first module, called the \emph{cross correlation I/O network}, given a set of input-output examples, produces a continuous representation of the set of I/O examples.
The second module, the \emph{Recursive-Reverse-Recursive Neural Network} (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs.~\citep{nsps}

% why types? differences from NLP, CS way, not reflected in ASTs in e.g. variables
% how to supervise with these? representation?

% section levels?

\subsection{Structure}

Type-based approaches to synthesis are based on \emph{deductive search}, a top-down search strategy.
Such type-based deductive search is most powerful in a setting where the underlying DSL is loosely-constrained,
that is, permits arbitrary type-safe combinations of subexpressions.
In particular, any ML-like calculus with algebraic datatypes can serve as a core language for type-driven synthesis.~\citep{gulwani2017program}

\subsection{Holes}

Particularly useful for our purposes is a programming language feature called \emph{typed holes}~\citep{hashimoto1997typed},
as used in Agda~\footnote{\url{https://agda.readthedocs.io/en/latest/getting-started/quick-guide.html}},
Idris~\footnote{\url{http://docs.idris-lang.org/en/latest/tutorial/typesfuns.html\#holes}},
and Haskell~\footnote{\url{https://wiki.haskell.org/GHC/Typed_holes}}.
Typed holes allow one to write an incomplete functional program template,
enabling type inference for any such holes, in turn enabling suggested completions to the user.
Such interactive program synthesis has been labeled as the solver-aided
programming method \emph{sketching}~\citep{gulwani2017program} or
\emph{type-driven development}.
% ~\footnote{\url{https://manning.com/books/type-driven-development-with-idris}}
As such, one of our goals is to improve suggested hole completions in such languages to facilitate this interactive programming style.

\subsection{Types}

To get the most out of our types, we will want to provide them for:
\begin{itemize}
    \item inputs
    \item outputs
    \item abstract syntax trees (ASTs)
\end{itemize}

The basic idea here is simple: our program should return the desired type, while taking the desired input types.
% BNFs

However, in the simple case, the implications of this info do not require much computation:
one will just restrict their search to those AST nodes allowed for the present hole as dictated by the grammar.

\subsection{Generics}

What makes the use of types more powerful in restricting the search space is the use of \emph{parametric polymorphism}:
a function $append$ may work using either lists of numbers or lists of strings.
~\footnote{Left out of scope here are \emph{refinement types}, which further aid synthesis based on conditions.}
As such, its type signature may be made \emph{generic} such as to have its return type reflect the types of the parameters used.
Having such information available at the type level may add additional information over what is used in the simpler case above.

It should be noted however that not all programs benefit from this,
but only those for which potential building blocks include statically typed functions with parametric return types.

% decouple synthesis/target languages
As a result, this restriction limits our method from being used for direct synthesis of programming languages that are dynamically typed,
e.g. Ruby language, as well as to those lacking parametric polymorphism, e.g. C language.
However, this only partly hinders those wishing to synthesize programs in such languages:
rather than synthesizing in such languages directly, we would suggest synthesizing from the language that best constrains the search space,
then using source-to-source translation methods (e.g. neural machine translation~\citep{kalchbrenner2013recurrent})
using the synthesized program as input to obtain a program translated into the target language.
% TODO: make into experiment hypothesis?

% I'd lean toward a language with ML framework as the source language (Python/Haskell/OCaml?), and a target language with holes (Haskell/Idris/Agda) and proper type-checks (any but Python/Prolog). none of these used any of those; this intersection means a shared source/target language implies Haskell is the only option and I need to throw out existing implementations anyway. in terms of respectable benchmark, L^2 seems to actually have been used by others before.

\subsection{Haskell language}

In other words, languages with stronger type systems are preferable for program synthesis.
This is well-known in the traditional program synthesis community,
where various researchers~\citep{synquid,hornclauses,scythe,gissurarson2018suggesting}
have been making use of the Haskell language,
a statically typed, purely functional programming language with type inference and lazy evaluation.~\footnote{\url{https://www.haskell.org/}}
% TODO: mention ML-like (= typed lisp) alternatives e.g. OCaml.

The advantages of Haskell as a synthesis language over the above-mentioned ML lies in its focus on the functional paradigm,
unlike e.g. OCaml: this paradigm is more amenable to static types, which for synthesis helps us constrain our search space.

Another advantage of Haskell over OCaml is it offers typed holes, which are useful for our purposes to evaluate the type of a given hole.
While Idris language offers these benefits as well, Haskell's lazy evaluation seems a more sensible default for our purposes than Idris's strict evaluation.
% TODO: justify why
~\footnote{As an interesting coincidence, using Haskell for program synthesis also means using an implementation of \citet{lambdacalculus}'s lambda calculus to address \citet{church1957applications}'s problem of synthesizing programs.}

% TODO: explain why I don't feel I need imperative statements or variable definitions
The reason we consider programs of a tree-based form, rather than as a list of imperative statements such as variable definitions,
is that the view of programs as function compositions guarantees us that any complete program will yield us output of the desired type.
This means we view our programs as \emph{pure functions}~\citep{fortran95},
i.e. returning a deterministic output for any given inputs, without performing any additional side effects.
This guarantees that, rather than just branching out, our search will focus on finding acceptable solutions.
This is to be contrasted with \emph{imperative} programs, a coding style characterized by variable mutation,
historically popularized for the purpose of performance optimization from the earlier languages such as assembly.
Synthesis for such languages exists as well~\citep{shi2019frangel}, but is generally harder as it cannot make as much use of types.
% TODO: explain partial programs, how types enable evaluation of these, and why this means we should perform program synthesis based on types

Whereas modern programming languages might have a broad plethora of grammatical constructs available,
for the purpose of our proof-of-concept we will opt to hide much of this.
To achieve this, we will take inspiration from the lambda calculus~\citep{lambdacalculus},
which opts to view constructs as functions,
both so as to provide a unified view of various operations, as well as to enable \emph{currying} of functions.
A \emph{curried}~\citep{currying} version of a function is not unlike its original form,
yet allowing arguments to the function to be applied to it one at a time.
The way this works is that, when an argument is applied to a curried form of a function taking two parameters,
the result is a function that still takes one parameter, before yielding the actual result of the original function.
As such, viewing traditional operators such as addition as curried functions can both simplify the way we view things,
while also increasing expressiveness.
And in fact, this is indeed what happens in various functional languages,
where traditional operators such as \verb|+| are viewed simply as infix operator forms of curried functions.
In Haskell language, for example, \verb|+| may be used to refer to the addition operator in infix notation,
whereas \verb|(+)| may be used to refer to its form as a traditional function.
Other languages such as OCaml, Idris and Agda follow a similar pattern.

As such, we might express as curried functions not only basic mathematical operators,
but by wrapping them as library functions,
also traditional grammatical constructs as (functional) conditionals and function composition operators.
Whereas for legibility purposes, a programmer may prefer to read code using a grammar rich with syntactical sugar,
for the purpose of program synthesis,
it is preferable to decouple the syntax used for synthesis from that presented to the reader,
so as to prevent duplication during synthesis to restrict the search as much as possible.
Instead, a synthesized program might then be converted so as to use a more legible grammar.
For the purpose of this thesis, we will not focus on reader-friendly syntax, however.

Grammar-wise, this should leave us with few options:
% referencing variables (whether from our library or parameters),
referencing variables from our library,
and applying arguments to functions.
For the sake of simplicity,
we will attempt to leave out certain common language features such as defining variables and creating lambda functions.
% ^ TODO: update statement on lambda functions
Hopefully, our idea can be demonstrated without this additional complexity.

To simplify the problem, we would furthermore disallow the use of arbitrary values such as free-style strings.
% Note: a proper solution here probably involves using e.g. https://hackage.haskell.org/package/ad to fix constants by backprop.

As a result, our Haskell-based grammar ends up looking as follows:
\begin{tabular}{ccccc}
    type & constructor & Type Parameters & example & note \\
    Exp & App & Exp Exp & exp exp & \\
    Exp & ExpTypeSig & Exp Type & exp \textbf{::} type & \\
    Exp & Paren & Exp & \textbf{(}exp\textbf{)} & \\
    Exp & Var & QName & qname & \\
    Type & TyApp & Type Type & Type Type & \\
    Type & TyCon & QName & QName & \\
    Type & TyFun & Type Type & Type \textbf{->} Type & \\
    Type & TyVar & Name & name & \\
    QName & UnQual & Name & name & redundant \\
    QName & Special & SpecialCon & specialCon & redundant \\
    Name & Ident & & ident & \\
    SpecialCon & ExprHole & & \textbf{\_} & \\
    SpecialCon & ListCon & & \textbf{[]} & \\
    Exp & Lambda & [Pat] Exp & \textbf{\textbackslash} patt1 patt2 ... \textbf{->} exp & lambdas \\
    Pat & PVar & Name & name & lambdas, redundant \\
    Exp & Let & Binds Exp & \textbf{let} binds \textbf{in} exp & lambdas typed without ScopedTypeVariables \\
    Binds & BDecls & [Decl] & decl1\textbf{;} decl2\textbf{;} ... & lambdas typed without ScopedTypeVariables \\
    Decl & PatBind & Pat Rhs (Maybe Binds) & pat \textbf{=} rhs & lambdas typed without ScopedTypeVariables \\
    Rhs & UnGuardedRhs & Exp & exp & lambdas typed without ScopedTypeVariables, redundant \\
\end{tabular}
% TODO: use BNF?
% TODO: make this not specific to haskell by simplifying out the haskell-specific bits redundant here in my simple version

Our programs are structured as follows:

\begin{verbatim}
    let
        foo = expr
        bar = expr
        ...
    in body
\end{verbatim}

\section{Experiment} % \label{sec:experiment}

\subsection{Benchmark task}

% benchmark task:
% - [x] typed MIL: Prolog/Python, types: poly list, mono int/char. allows recursion.
% - [x] Myth: Haskell/OCaml, types: poly list/tree, mono bool/nat. third-party repo available but errors. partial official code. allows recursion. components not listed in the paper~\citep{tamandu}.
% - [x] lambda^2: OCaml/Python, types: poly list/tree, mono bool/num. permissive grammar involving pattern matching, recursion, and a flexible set of primitive operators and constants.
% - [x] NPI/AlphaNPI: task is sequential :(
% - [x] NSPS: flashfill so few parametric types :(, also tasks not publicized
% - DILP?: paper tests induction rather than synthesis, so adapting their benchmark makes it hard to compare -- just rules, no trees or types :(
% - [x] DeepCoder: dataset (/code) not published :(
% - [x] Suggesting Valid Hole Fits: based on types, not examples :(
% - [x] Synquid / Polymorphic Succinct Types: based on refinement types instead of i/o examples :(
% - [x] Scythe: no code :(, uses types + i/o examples. no (own) task! :(
% - [x] Scout: no code, and only extended abstract available, with no info on task :(
% - [x] Idris: based on types, not examples :(
% - [x] Houdini: tasks mostly involve neural networks so suck to evaluate :(
% - [x] Terpret: 12 tasks (table 2), complex grammar (figure 3) incl constants on probabilistic language, so seems kinda different, plus quite binary so not very type-ey :(
% - [x] Tamandu: PBE with type-based pruning, but non-neural, so only evaluated on 23 functions. allows recursion. types so simple I can't add value. :(
% https://docs.google.com/spreadsheets/d/1uDA9suwASDzllxJZDt--wZ0ci7q4eJIfPcAw9qr18-U/edit?usp=sharing

We had trouble finding suitable benchmark task in the literature.

On the one hand, especially benchmark tasks within non-neural program synthesis methods have usually consisted of only a limited number of tasks%
~\citep{myth,lambda2,typedmil,houdini,tamandu,dilp}~\cite{terpret},
as such non-neural methods do not require an equivalent to the training set typically used in machine learning setups.

% While \cite{lambda2} did not use neural methods,
On the other hand, benchmark tasks within Neural Program Synthesis have often remained somewhat simpler in nature,
rendering them less fit for our purposes --- either being imperative in nature (and as such being less amenable to types),
such as sorting tasks~\citep{npi,alphanpi},
or otherwise focusing on a simple set of types --- such as strings~\citep{nsps} ---%
or differently put, not including building blocks with parametric return types.

As a third issue, it was also common for existing papers to keep their datasets unpublished~\citep{nsps,deepcoder}.

As a result, it looks like we would need to create a benchmark task, or if anything a training dataset compatible with one of the existing benchmark tasks, ourselves.

Now, for compatibility we may simply ensure our test sets contain the benchmark tasks of existing papers.
% To achieve this, we may increasingly add types, such as to support, in order of complexity:
% \begin{itemize}
%     \item list, bool~\citep{terpret}
%     \item int~\citep{tamandu}
%     \item char~\citep{typedmil}
%     \item nat, tree~\citep{myth,lambda2}
% \end{itemize}

To constrain the engineering scope of our experiment, it would seem reasonable to use a benchmark paper similarly limited in grammar, so as to reduce complexity not required for the experiment where possible.

% By this reasoning, we would like to benchmark against \citet{tamandu}, which "[focuses] on the synthesis of purely functional programs without any lambda expressions, explicit recursion or conditionals (i.e., only function application is allowed)".

% Other papers considered as potential benchmarks often featured more comprehensive grammars, typically incorporating function recursion~\citep{typedmil,myth}, if not also pattern matching and a flexible set of primitive operators and constants~\citep{lambda2}.

We will do this by taking libraries of basic operations in accordance with these papers, then, in order to create a training set, generating any possible functions using combinations thereof within a given complexity threshold.

The hole filling is as follows:
\begin{itemize}
    % TODO: handle parameters ensuring their use
    \item we take our function, containing a number of parameters, a return type, and a body that contains holes;
    \item we enumerate the variables we have that could (if applicable, after function application) return a type compatible with our desired return type, to create versions of the function with said hole plugged with these variables (which in the case of functions, may, in turn, contain holes);
    % TODO: queue or whatever, any data structure really
    % TODO: distinguish unfilled currying slot vs unfilled holes
    \item we filter these potential programs given their number of remaining holes based on our complexity threshold;
    % TODO: specify complexity threshold
    \item we add the remaining programs to a queue of complete or partial programs, depending on whether they still contain holes;
    \item we similarly process the functions in the queue of partial programs, until we are left with an enumeration of complete programs;
    \item we evaluate the complete programs by the input-output examples, to filter down to those matching our behavioral criteria;
    \item finally, we generate sample inputs for each program, and calculate their corresponding outputs.
    % TODO: consider a depth-first alternative for lower memory complexity
    % TODO: compare to approach used in https://github.com/minori5214/programming-by-example/blob/master/generate.py
\end{itemize}

The process for generating task functions to train our synthesizer on is as follows:
\begin{itemize}
    \item we start using a body consisting of a hole, typed with a wildcard;
    \item we use hole-filling to generate potential complete programs within a certain complexity threshold.
    % TODO: compare to approach used in https://github.com/minori5214/programming-by-example/blob/master/generate.py
\end{itemize}

We additionally considered alternative methods of generating task functions:
\begin{itemize}
    \item generating a function based on a type signature;
    \item generating a function based on an input parameter type; % , then filling holes, only afterwards seeing what return type ends up coming out.    
\end{itemize}
There are two reasons we decided against using these type-first approaches:
\begin{itemize}
    \item as they impose constraints on the functions generated, they will end up generating less functions given the same amount of compute;
    \item if we wanted to allow generating more general versions of the desired function, then pre-fixing the types such as not to allow more generic implementations feels counter-productive, if not anti-thetical to the spirit of functional programming.
\end{itemize}

The synthesis process itself is as follows:
\begin{itemize}
    \item we take the type of our function, along with a body consisting of a hole;
    \item we use hole-filling to generate potential complete programs;
    \item we generate sample inputs for each program, and calculate their corresponding outputs;
    \item we evaluate the complete programs by the input-output examples, to filter down to those matching our behavioral criteria;
    % TODO: consider a depth-first alternative for lower memory complexity
\end{itemize}

% TODO: EXPERIMENT
% TODO: elaborate how to supervise existing methods with type info

\section{Result} % \label{sec:result}

Having added our type-level supervision during training, we expect synthesis success rates to rise and required evaluations to drop compared to the baseline algorithm.
This demonstrates that the findings from traditional program synthesis methods are relevant also in the field of neural program synthesis.

% TODO: RESULT

\section{Discussion} % \label{sec:discussion}

One question we aim to answer here is whether our approach can meaningfully scale to program sizes not explicitly train on.
% TODO: check what claims NSPS made on this.

\subsection{Topics for future research}
\begin{itemize}
    \item How can we apply a more generalized program synthesis model to a specific problem? Can we take inspiration here from neural network pruning?
    % \item ...
\end{itemize}

\nocite{*}
% \bibliographystyle{acm}
\bibliographystyle{plainnat}  % fixes https://tex.stackexchange.com/questions/166840/why-do-i-get-author-when-i-use-citet-with-natbib
\bibliography{references}

\end{document}
